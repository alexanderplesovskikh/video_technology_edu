import random
import uuid
import logging
from datetime import datetime
import zulip
import os
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
import faiss
from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain_core.documents import Document
import tqdm
from langchain_ollama.llms import OllamaLLM
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
import re
import torch
import gc
from langchain_core.prompts import ChatPromptTemplate

from db.events import store_event

import json
import os
from datetime import datetime

def escape_all_variables(template: str) -> str:
    """
    Escape all variables in the template string.

    Args:
    - template (str): The template string containing variables to escape.

    Returns:
    - str: The template string with all variables escaped.
    """
    # Use regex to replace {var} with {{var}} for any variable
    escaped_template = re.sub(r'\{([^}]+)\}', r'{{\1}}', template)
    return escaped_template

# Define the path to your JSON file
json_file_path = '/home/user/record_bot_data.json'

# Function to append data to the JSON file
def append_to_json_file(file_path, new_data):
    # Check if the file exists
    if os.path.exists(file_path):
        # Open the existing JSON file and load its content
        with open(file_path, 'r') as file:
            try:
                data = json.load(file)
            except json.JSONDecodeError:
                data = []  # If the file is empty or not valid JSON, start with an empty list
    else:
        # If the file does not exist, create an empty list
        data = []

    # Append the new data to the list
    data.append(new_data)

    # Write the updated data back to the JSON file
    with open(file_path, 'w') as file:
        json.dump(data, file, indent=4)


gc.collect()
torch.cuda.empty_cache()

token_speed = 150

base_dir = os.path.dirname(os.path.abspath(__file__)) + '/'

print('model start loading')
model_ollama = "gemma2:latest"
#best model but we need more memory
model_ollama = "electromagneticcyclone/t-lite-q:3_k_m"
model_ollama = "llama3.1:8b-instruct-q4_K_S"
model_ollama = "owl/t-lite:latest"

init_ollama = OllamaLLM(
    model=model_ollama,
    temperature = 0.3,
    streaming=True,
    num_ctx=4096,
    callbacks=[StreamingStdOutCallbackHandler()],
)

#max_tokens=10,
#num_predict=10,

print('model end loading')

user_level = ''
user_theme = ''
user_id = -1

def extract_number(file_name):
    # Use regex to find the number in the file name
    match = re.search(r'(\d+)', file_name)
    return int(match.group(1)) if match else float('inf')  # Return a large number if no digit found

def check_folder_exists(folder_path):
    if os.path.exists(folder_path) and os.path.isdir(folder_path):
        print(f"The folder '{folder_path}' exists.")
        return True
    else:
        print(f"The folder '{folder_path}' does not exist.")
        return False

def get_chuncks(text_string, chunkssize, chunkoverlap, lengthfunc):
    text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=chunkssize,
    chunk_overlap=chunkoverlap,
    length_function=lengthfunc,
    is_separator_regex=False,)

    texts = text_splitter.create_documents([text_string])

    res = [text.page_content for text in texts if len(text.page_content) > 300]
    return res

def read_first_lines(folder_path):
    first_lines = []
    all_lines = {}
    # Iterate through all files in the folder
    for file_name in sorted(os.listdir(folder_path), key=extract_number):
        # Check if the file has a .txt extension
        if file_name.endswith(".txt"):
            file_path = os.path.join(folder_path, file_name)

            # Open the file and read the first line
            with open(file_path, 'r', encoding='utf-8') as file:
                all_current_lines = file.readlines()
                if file_path in all_lines:
                    all_lines[file_path] += all_current_lines
                else:
                    all_lines[file_path] = all_current_lines


                first_line = all_current_lines[0].strip()  # Read and strip any trailing spaces/newlines
                print("file_path: ", file_path)
                print("first line: ", first_line)
                if first_line[0] == "#":
                    first_line = first_line[1:].strip()

                first_lines.append(first_line)

    return first_lines, all_lines

# Example usage
folder_path = base_dir + "course_materials"
lines, all_lines = read_first_lines(folder_path)

print(lines)
print(all_lines.keys())

lines_to_str = ''

all_theme_ids = []

for line_id in range(len(lines)):
    all_theme_ids.append(int(line_id+1))
    lines_to_str += str(line_id + 1) + ". " + lines[line_id] + "\n"


main_menu = f"""‚úã –ü—Ä–∏–≤–µ—Ç! –Ø –±–æ—Ç-–æ–±—É—á–∞—Ç–æ—Ä –ø–æ –í–∏–¥–µ–æ—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è–º üé•\n\n
–ü–æ–º–æ–≥—É —Ç–µ–±–µ —Ä–∞–∑–æ–±—Ä–∞—Ç—å—Å—è –≤–æ –≤—Å–µ—Ö –¥–µ—Ç–∞–ª—è—Ö –∫—É—Ä—Å–∞. **–î–ª—è –Ω–∞—á–∞–ª–∞ –≤—ã–±–µ—Ä–∏ —Ç–µ–º—É, –∫–æ—Ç–æ—Ä–∞—è —Ç–µ–±–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–∞:**\n
{lines_to_str}
\n
**–û—Ç–ø—Ä–∞–≤—å –º–Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å–≤—É—é—â–∏–π –Ω–æ–º–µ—Ä —Ç–µ–º—ã, –Ω–∞–ø—Ä–∏–º–µ—Ä**: **```1```** –∏–ª–∏ **```5```**\n
\n---\n
–ß—Ç–æ–±—ã –≤–µ—Ä–Ω—É—Ç—å—Å—è –≤ –≥–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é, –≤–≤–µ–¥–∏: **`–ø–æ–º–æ—â—å`**
"""

# Example usage
folder_path_faiss = base_dir + "faiss"
folder_exists = check_folder_exists(folder_path_faiss)

vs_name = base_dir + 'faiss/faiss_1_1'
embedding_function = HuggingFaceEmbeddings(model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')

if not folder_exists:
    print('start to create faiss')
    res = {}
    for key in all_lines:
        temp_res = ''
        for itemm in all_lines[key]:
            temp_res += itemm
        if key in res:
            res[key] += temp_res
        else:
            res[key] = temp_res


    chunk_dict = {}
    for key in res:
        res_chunks = get_chuncks(res[key], 500, 50, len)
        if key in chunk_dict:
            chunk_dict[key] += res_chunks
        else:
            chunk_dict[key] = res_chunks

    chunk_key_pairs = []
    for key in chunk_dict:
        for chunk in chunk_dict[key]:
            chunk_key_pairs.append([key, chunk])



    embeddings = embedding_function.embed_documents('–≠—Ç–æ –ø—Ä–∏–º–µ—Ä –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.')
    index = faiss.IndexFlatL2(len(embeddings[0]))
    embed_dim = len(embeddings[0])
    vector_store = FAISS(
            embedding_function=embedding_function,
            index=index,
            docstore=InMemoryDocstore(),
            index_to_docstore_id={},
        )

    list_langchain_docs = []
    for element in tqdm.tqdm(chunk_key_pairs):
        key, chunk = element[0], element[1]
        list_langchain_docs.append(Document(page_content=chunk, metadata={"source": key},))

    uuids = [i+1 for i in range(len(list_langchain_docs))]

    print("uuids: ", uuids)

    vector_store.add_documents(documents=list_langchain_docs, ids=uuids)
    vector_store.save_local(vs_name)

new_vector_store = FAISS.load_local(
    vs_name, embedding_function, allow_dangerous_deserialization=True
)


def search_chunks(query,k=5):
    results = new_vector_store.similarity_search(
    query,
    k=k
    )

    gotten_chunks = []
    for res in results[:-1]:
        if " - " + res.page_content not in gotten_chunks:
            gotten_chunks.append(" - " + res.page_content)

    context_faiss = "\n".join(chunk for chunk in gotten_chunks)
    return context_faiss

def search_chunks_new_knowledge(query,k=5):
    results = new_vector_store.similarity_search(
    query,
    k=k
    )

    to_put = " - " + results[-1].page_content

    context_faiss = to_put

    return context_faiss 

def search_chunks_biblio(query,k=5):
    results = new_vector_store.similarity_search(
    query,
    k=k
    )

    gotten_chunks = []
    for res in results[:-1]:
        if " - " + res.page_content not in gotten_chunks:
            source_path = res.metadata['source']
            source_file = source_path.split("/")[-1].split(".txt")[0].strip()
            line_index = int(source_file) - 1
            source_line = lines[line_index]
            gotten_chunks.append(" - " + res.page_content + f"""\n*¬© –ò—Å—Ç–æ—á–Ω–∏–∫: {source_line}*\n---\n""")

    context_faiss = "\n".join(chunk for chunk in gotten_chunks)

    context_faiss = context_faiss.replace(":::", "")
    return context_faiss

def format_llm_prompt(query):
    response = init_ollama.invoke(query)
    return response
    #torch.cuda.empty_cache()
    #prompt = ChatPromptTemplate.from_template(repr(json.dumps(json.dumps(str(query)))))
    #chain = prompt | init_ollama
    #return chain.stream({'key': 'value'})


user_states = {}

user_history = {}

class QuizBot:
    def initialize(self, bot_handler):
        self.bot_handler = bot_handler
        self.setup_logging()
        self.client = zulip.Client(config_file=base_dir + "zuliprc")

    def setup_logging(self):
        logging.basicConfig(level=logging.DEBUG, format='%(asctime)s %(levelname)s %(message)s')

    def send_reply(self, message, response):
        res = store_event(recipient=message["sender_email"], content=response, operation_type='send')
        return res
    
    def update_reply(self, message, response, get_event_id):
        res = store_event(recipient=message['sender_email'], content=response, operation_type='update', updating_event_id=get_event_id)
        return res

    def usage(self) -> str:
        pass

    def handle_message(self, message, bot_handler):

        user_id = message["sender_id"]
        content = message["content"].strip().lower()

        if user_id not in user_history:
            user_history[user_id] = []

        if user_id not in user_states:
            user_states[user_id] = {"state": "main_menu"}

        state = user_states[user_id]["state"]

        if content in ["–ø–æ–º–æ—â—å", 'help', 'start', 'exit']:
            user_states[user_id] = {"state": "main_menu"}
            self.send_reply(message, main_menu)
            return

        if state == "main_menu":
            if re.match(r"^\d+$", content.strip()) and int(content.strip()) in all_theme_ids:
                user_states[user_id] = {"state": "select_level", "topic": int(content)}
                self.send_reply(message, "–ö–∞–∫–æ–π —É –≤–∞—Å —É—Ä–æ–≤–µ–Ω—å –∑–Ω–∞–Ω–∏–π –ø–æ —Ç–µ–º–µ? –í—ã–±–µ—Ä–∏, –≤–≤–µ–¥—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é —Ü–∏—Ñ—Ä—É: \n**`1. –Ω–∞—á–∞–ª—å–Ω—ã–π`**\n**`2. —Å—Ä–µ–¥–Ω–∏–π`**\n**`3. –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π`**\n\n–ß—Ç–æ–±—ã –≤–µ—Ä–Ω—É—Ç—å—Å—è –≤ –≥–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é, –≤–≤–µ–¥–∏: **`–ø–æ–º–æ—â—å`**")
            else:
                self.send_reply(message, main_menu)

        elif state == "select_level":
            if content in ["1", "2", "3"]:
                user_states[user_id] = {"state": "chat", "topic": user_states[user_id]["topic"], "level": content}
                #open file
                #prompt model to summorize it
                topic = user_states[user_id]["topic"]

                user_history[user_id] = []

                file_path_summary = base_dir + 'course_materials/' + str(topic) + ".txt"

                with open(file_path_summary, 'r', encoding="utf-8") as sum_file:
                    sum_lines = sum_file.readlines()

                sum_firstlines = sum_lines[0]

                if sum_firstlines[0] == "#":
                    sum_firstlines = sum_firstlines[1:].strip()

                sum_restlines = " ".join(sum_lines[1:])

                summary_prompt = f"""- –¢–µ–∫—Å—Ç: {sum_restlines}.
- –ó–∞–¥–∞–Ω–∏–µ: —Ç—ã –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, –Ω–∞–ø–∏—à–∏ –∫–æ—Ä–æ—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –¥–∞–Ω–Ω–æ–≥–æ —Ç–µ–±–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è —Ç–µ–º—ã –∫—É—Ä—Å–∞, –∫—Ä–∞—Ç–∫–æ –ø–æ –ø—É–Ω–∫—Ç–∞–º –æ–±–æ–∑–Ω–∞—á—å –æ—Å–Ω–æ–≤–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã, —á–µ—Ä–µ–∑ –∑–Ω–∞–∫ "-".
- –ù–µ –ø–∏—à–∏ –º–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞, –ø—Ä–æ—Å—Ç–æ –ø—Ä–∏–≤–µ–¥–∏ –ø–æ–¥—Ç–µ–º—ã –¥–∞–Ω–Ω–æ–≥–æ —Ç–µ–±–µ –º–∞—Ç–µ—Ä–∏–∞–ª–∞ –ø–æ –∫—É—Ä—Å—É, —á–µ—Ä–µ–∑ "-", –∫–∞–∂–¥—ã–π –ø—É–Ω–∫—Ç –Ω–∞—á–∏–Ω–∞–π —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏.
- –í –∫–æ–Ω—Ü–µ —Ä–µ–∑—é–º–µ —Å –Ω–æ–≤–æ–≥–æ –∞–±–∑–∞—Ü–∞, —Å–ø—Ä–æ—Å–∏ –ø—Ä–æ —á—Ç–æ-—Ç–æ –æ–¥–Ω–æ –∏–∑ —Ä–µ–∑—é–º–µ - –∑–∞–¥–∞–π –æ—Ç–∫—Ä—ã—Ç—ã–π –≤–æ–ø—Ä–æ—Å –ø–æ —Ç–∏–ø—É: –ê –∑–Ω–∞–µ—à—å –ª–∏ —Ç—ã, —á—Ç–æ —Ç–∞–∫–æ–µ ..."""

                self.send_reply(message, "üë®‚Äçüè´ –î–∞–≤–∞–π –ø–æ—Å–º–æ—Ç—Ä–∏–º, —á—Ç–æ –Ω–∞–º –ø—Ä–µ–¥—Å—Ç–æ–∏—Ç —É–∑–Ω–∞—Ç—å –≤ —Ä–∞–º–∫–∞—Ö —ç—Ç–æ–π —Ç–µ–º—ã.")

                llm_summary = format_llm_prompt(summary_prompt)

                user_states[user_id]['init_message'] = ""
                user_states[user_id]['last_updated_messsage'] = ""


                user_states[user_id]['init_message'] = f"–ü—Ä–æ–≤–µ–¥—É —Ç–µ–±–µ –∫—Ä–∞—Ç–∫–∏–π —ç–∫—Å–∫—É—Ä—Å –ø–æ —Ç–µ–º–µ: **{sum_firstlines}**\n---\n"

                event_id = self.send_reply(message, user_states[user_id]['init_message'])

                if event_id:
                    count = 0
                    for token in llm_summary:
                        user_states[user_id]['init_message'] += token
                        count+= 1 
                        if count % token_speed == 0:
                            self.update_reply(message, user_states[user_id]['init_message'], event_id)
                            user_states[user_id]['init_message'] = user_states[user_id]['init_message']
                            
                    
                    if user_states[user_id]['init_message'] != user_states[user_id]['last_updated_messsage']:
                        self.update_reply(message, user_states[user_id]['init_message'], event_id)

                    ending_message = "\n---\n**‚ùì –î–∞–≤–∞–π –Ω–∞—á–Ω—ë–º –∏–∑—É—á–µ–Ω–∏–µ —Ç–µ–º—ã! –ú–æ–∂–µ—à—å –∑–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å –ø–æ –ª—é–±–æ–º—É –∞—Å–ø–µ–∫—Ç—É —Ç–µ–º—ã, —è –æ—Ç–≤–µ—á—É —Å –æ–ø–æ—Ä–π –Ω–∞ –º–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–∞ –°–µ—Ç–µ–≤—ã—Ö –≤–∏–¥–µ–æ—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –∏–ª–∏ –º–æ–∂–µ–º –Ω–∞—á–∞—Ç—å –±–µ—Å–µ–¥—É –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–º—É –º–Ω–æ–π –≤–æ–ø—Ä–æ—Å—É ‚Äî –≤ –ª—é–±–æ–º —Å–ª—É—á–∞–µ –≤—ã–±–∏—Ä–∞—Ç—å —Ç–µ–±–µ! –ü—Ä–æ—Å—Ç–æ `–Ω–∞–ø–∏—à–∏ —Å–≤–æ–π –≤–æ–ø—Ä–æ—Å` –∏–ª–∏ –æ—Ç–ø—Ä–∞–≤—å –º–Ω–µ: `–†–∞—Å—Å–∫–∞–∂–∏, —á—Ç–æ —Ç–∞–∫–æ–µ <...>` ‚Äî –∏ –º—ã –Ω–∞—á–Ω–µ–º –±–µ—Å–µ–¥—É üòâ**\n---\n–î–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞ –≤ –º–µ–Ω—é –≤–≤–µ–¥–∏—Ç–µ **`–ø–æ–º–æ—â—å`**."
                    self.update_reply(message, user_states[user_id]['init_message'] + ending_message, event_id)

                    if len(user_history[user_id]) > 10:
                        del user_history[user_id][0]

                    user_history[user_id].append({"role": "AI-assistant", "message": user_states[user_id]['init_message']})
            
            else:
                self.send_reply(message, "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—ã–±–µ—Ä–∏—Ç–µ —É—Ä–æ–≤–µ–Ω—å, –≤–≤–µ–¥—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é —Ü–∏—Ñ—Ä—É: \n**`1. –Ω–∞—á–∞–ª—å–Ω—ã–π`** \n**`2. —Å—Ä–µ–¥–Ω–∏–π`** \n**`3. –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π`**\n\n–ß—Ç–æ–±—ã –≤–µ—Ä–Ω—É—Ç—å—Å—è –≤ –≥–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é, –≤–≤–µ–¥–∏: **`–ø–æ–º–æ—â—å`**")

        elif state == "chat":
            topic = user_states[user_id]["topic"]
            level = user_states[user_id]["level"]
            topic_name = lines[int(str(topic).strip())-1]

            model_level_desc = ''
            if str(level).strip() == '1':
                model_level_desc = '–æ–±—ä—è—Å–Ω–µ–Ω–∏—è –ø–æ–ø—Ä–æ—â–µ —Å –ø—Ä–æ—Å—Ç–æ–π –ª–µ–∫—Å–∏–∫–æ–π –∏ –ø–æ–∫—Ä–æ—á–µ'
            if str(level).strip() == '2':
                model_level_desc = '—Å—Ä–µ–¥–Ω–∏–π —É—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –ª–µ–∫—Å–∏–∫–∏ –∏ —Å—Ä–µ–¥–Ω–∏–π —É—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏–π'
            if str(level).strip() == '3':
                model_level_desc = '—Å–ª–æ–∂–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å –ª–µ–∫—Å–∏–∫–∏ –Ω–∞—É—á–Ω—ã–π –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–π —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–π —è–∑—ã–∫ –∏ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è'

            #Main answer

            chunks_for_llm = search_chunks(content)

            user_query = f"""# –ü–æ–º–Ω–∏, —á—Ç–æ –µ—Å—Ç—å –∏—Å—Ç–æ—Ä–∏—è —Ä–∞–∑–≥–æ–≤–æ—Ä–∞: {str(user_history[user_id])}

# –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –æ—Ç–≤–µ—Ç–∞:
- –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ –∫—É—Ä—Å—É: {chunks_for_llm}.

# –í–≤–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:
- –¢—ã –ø—Ä–µ–ø–æ–¥–æ–≤–∞—Ç–µ–ª—å-–ø—Ä–æ—Ñ–µ—Å—Å–æ—Ä –≤ –í—ã—Å—à–µ–π —à–∫–æ–ª–µ —ç–∫–æ–Ω–æ–º–∏–∫–∏ –∏ –∫ —Ç–µ–±–µ –æ–±—Ä–∞—â–∞—é—Ç—Å—è —Å—Ç—É–¥–µ–Ω—Ç—ã –ø–æ —Ç–µ–º–µ –∫—É—Ä—Å–∞ "{topic_name}".
- –¢—ã –¥–æ–ª–∂–µ–Ω –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å —Å—Ç—É–¥–µ–Ω—Ç—É –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ –ø–æ–¥–¥–µ—Ä–∂–∞—Ç—å –±–µ—Å–µ–¥—É –ø–æ —Ç–µ–º–µ –∫—É—Ä—Å–∞, –µ—Å–ª–∏ –≤–æ–ø—Ä–æ—Å–∞ –Ω–µ—Ç.
- –°—Ç—É–¥–µ–Ω—Ç —Ç–µ–±–µ –Ω–∞–ø–∏—Å–∞–ª: {content}.
- –î–ª—è –æ—Ç–≤–µ—Ç–∞ –∏—Å–ø–æ–ª—å–∑—É–π {model_level_desc}.
- –ü–û–ú–ù–ò, —á—Ç–æ –Ω—É–∂–Ω–æ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—É—é —Ç–µ–±–µ –ø–æ –∫—É—Ä—Å—É.
- –î–ª–∏–Ω–∞ —Ç–≤–æ–µ–≥–æ –æ—Ç–≤–µ—Ç–∞ –∏ —Ç–µ–∫—Å—Ç–∞ –Ω–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –±–æ–ª—å—à–µ 2-3 –∞–±–∑–∞—Ü–µ–≤.
# –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç–≤–µ—Ç–∞:
- –û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å —Å—Ç—É–¥–µ–Ω—Ç–∞ (–≤ —Ä–∞–º–∫–∞—Ö –∫—É—Ä—Å–∞) –∏–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ —Ä–∞–∑–≥–æ–≤–æ—Ä —Å–æ —Å—Ç—É–¥–µ–Ω—Ç–æ–º (–≤ —Ä–∞–º–∫–∞—Ö –∫—É—Ä—Å–∞) —Å–∞–º—ã–º –ª—É—á—à–∏–º –æ–±—Ä–∞–∑–æ–º, –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –∏ –ª–æ–≥–∏—á–Ω–æ.
- –¢–≤–æ–π –æ—Ç–≤–µ—Ç –ù–ï –î–û–õ–ñ–ï–ù –ü–†–ï–í–´–®–ê–¢–¨ –ø–æ –æ–±—ä–µ–º—É 2-3 –∞–±–∑–∞—Ü–µ–≤.
"""

            chunks_get = search_chunks_biblio(content)

            #user_states[user_id]['init_message'] = f"**Debug info:** –≤—ã –≤—ã–±—Ä–∞–ª–∏ —Ç–µ–º—É {topic}. {topic_name}. C —É—Ä–æ–≤–Ω–µ–º –∑–Ω–∞–Ω–∏–π: {level}. –í–∞—à –≤–æ–ø—Ä–æ—Å: {content}\n\nüí≠...\n\n"
            user_states[user_id]['init_message'] = f"üí≠...\n\n"
            user_states[user_id]['last_updated_messsage'] = ""

            event_id = self.send_reply(message, user_states[user_id]['init_message'])
            llm_main_response = format_llm_prompt(user_query)

            user_states[user_id]['to_be_checked_by_agent'] = ''

            if event_id:
                count = 0
                for token in llm_main_response:
                    user_states[user_id]['init_message'] += token
                    count+= 1 
                    user_states[user_id]['to_be_checked_by_agent'] += token
                    if count % token_speed == 0:
                        self.update_reply(message, user_states[user_id]['init_message'], event_id)
                        user_states[user_id]['init_message'] = user_states[user_id]['init_message']
                            
                    
                if user_states[user_id]['init_message'] != user_states[user_id]['last_updated_messsage']:
                    self.update_reply(message, user_states[user_id]['init_message'], event_id)

                
                response_ending = f"""\n```spoiler  üìö –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏:
{chunks_get}
```
\n
"""
                self.update_reply(message, user_states[user_id]['init_message'] + response_ending, event_id)

                if len(user_history[user_id]) > 10:
                    del user_history[user_id][0]

                user_history[user_id].append({"role": "AI-assistant", "message": user_states[user_id]['init_message']})
                

            else:
                self.send_reply(message, "Fatal error")

            #Agents start

            user_states[user_id]['init_message'] = "‚úÖ –ü–æ–¥–∫–ª—é—á–∞–µ–º –∞–≥–µ–Ω—Ç–æ–≤ –ø–æ –ø—Ä–æ–≤–µ—Ä–∫–µ –æ—Ç–≤–µ—Ç–æ–≤...\n"
            user_states[user_id]['last_updated_messsage'] = ""

            event_id = self.send_reply(message, user_states[user_id]['init_message'])

            agent_prompt = f"""### –ü–æ–º–Ω–∏, —á—Ç–æ –µ—Å—Ç—å –∏—Å—Ç–æ—Ä–∏—è —Ä–∞–∑–≥–æ–≤–æ—Ä–∞: {str(user_history[user_id])}
            
### –¢–µ–∫—Å—Ç: 
{user_states[user_id]['to_be_checked_by_agent']}
### –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è, –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ—Ç–æ—Ä–æ–π –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ç–µ–∫—Å—Ç:
{chunks_for_llm}

### –ó–∞–¥–∞–Ω–∏–µ:
- –¢—ã –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ –∫—É—Ä—Å—É –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–∞—è –≥—Ä–∞—Ñ–∏–∫–∞. –°–ª–µ–¥—É–π —Å–ª–µ–¥—É—é—â–µ–π –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:
- –ù–µ–æ–±—Ö–æ–¥–∏–º–æ, —á—Ç–æ–±—ã —Ç—ã –ø—Ä–æ–≤–µ—Ä–∏–ª —Ç–µ–∫—Å—Ç –Ω–∞ –µ–≥–æ —Å–æ–æ—Ç–≤–µ—Ç—Å–≤–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –ü—Ä–æ–≤–µ—Ä—å, —á—Ç–æ —Ç–µ–∫—Å—Ç –±—ã–ª –Ω–∞–ø–∏—Å–∞–Ω —Å –æ–ø–æ—Ä–æ–π –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∞ –¥–∞–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é. –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å–≤—É–µ—Ç –∑–∞–¥–∞–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, —Ç–æ —Ç—ã –¥–æ–ª–∂–µ–Ω –∏—Å–ø—Ä–∞–≤–∏—Ç—å —Ç–µ–∫—Å—Ç —Ç–∞–∫, —á—Ç–æ–±—ã –≤ –Ω–µ–º –±—ã–ª–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –¢–û–õ–¨–ö–û –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏ –ù–ò–ö–ê–ö–û–ô –î–†–£–ì–û–ô –í–ù–ï–®–ù–ï–ô –ò–ù–§–û–†–ú–ê–¶–ò–ò.
- –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç–≤–µ—Ç–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Å–ª–µ–¥—É—é—â–µ–π: –≤ –æ—Ç–≤–µ—Ç–µ –Ω–∞–ø–∏—à–∏ –¢–û–õ–¨–ö–û –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Å –æ–ø–æ—Ä–æ–π —Ç–æ–ª—å–∫–æ –Ω–∞ –¥–∞–Ω–Ω—É—é —Ç–µ–±–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.
- –¢–µ–∫—Å—Ç —è–≤–ª—è–µ—Ç—Å—è –æ—Ç–≤–µ—Ç–æ–º –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π –≤–æ–ø—Ä–æ—Å: {content}.
- –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –ù–ï –¥–æ–ª–∂–µ–Ω –ø—Ä–µ–≤—ã—à–∞—Ç—å –ø–æ –æ–±—ä–µ–º—É 2-3 –∞–±–∑–∞—Ü–µ–≤.
- –û—Ç–ø—Ä–∞–≤—å –º–Ω–µ –¢–û–õ–¨–ö–û –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç, –Ω–µ –ø—Ä–∏–≤–æ–¥–∏ –º–Ω–µ –Ω–∏–∫–∞–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π.
"""

            llm_agent = format_llm_prompt(agent_prompt)

            user_states[user_id]['agents_get_response'] = ''

            if event_id:
                count = 0
                for token in llm_agent:
                    user_states[user_id]['init_message'] += token
                    user_states[user_id]['agents_get_response'] += token
                    count+= 1 
                    if count % token_speed == 0:
                        self.update_reply(message, user_states[user_id]['init_message'], event_id)
                        user_states[user_id]['init_message'] = user_states[user_id]['init_message']
                            
                    
                if user_states[user_id]['init_message'] != user_states[user_id]['last_updated_messsage']:
                    self.update_reply(message, user_states[user_id]['init_message'], event_id)

                '''agent_mes_ending = f"""\n```spoiler  ‚õî Debug info:
{agent_prompt}
```
"""'''
                agent_mes_ending = ""

                self.update_reply(message, user_states[user_id]['init_message'] + agent_mes_ending, event_id)

                if len(user_history[user_id]) > 10:
                    del user_history[user_id][0]

                user_history[user_id].append({"role": "AI-assistant", "message": user_states[user_id]['init_message']})
            
            else:
                self.send_reply(message, "Fatal error")

            #Question back
            user_states[user_id]['init_message'] = "üìù –§–æ—Ä–º–∏—Ä—É—é –ø–ª–∞–Ω –æ–±—Å—É–∂–¥–µ–Ω–∏—è...\n"
            user_states[user_id]['last_updated_messsage'] = ""

            event_id = self.send_reply(message, user_states[user_id]['init_message'])

            new_know_chunks = search_chunks_new_knowledge(content)
            new_query = f"""### –í–æ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è: {new_know_chunks}. ### –ó–∞–¥–∞–Ω–∏–µ: —Å—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π –æ–¥–∏–Ω –æ—Ç–∫—Ä—ã—Ç—ã–π –≤–æ–ø—Ä–æ—Å –ø–æ –¥–∞–Ω–Ω–æ–π —Ç–µ–±–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –≤ –æ—Ç–≤–µ—Ç–µ —É–∫–∞–∂–∏ —Ç–æ–ª—å–∫–æ —Å–∞–º –≤–æ–ø—Ä–æ—Å. –ù–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –ø—ã—Ç–∞—Ç—å—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤—Å–µ —Ñ–∞–∫—Ç—ã –∏–∑ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –º–æ–∂–Ω–æ –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å—Å—è –ª–∏—à—å —á–∞—Å—Ç—å—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –ø–æ –Ω–µ–π —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –æ–¥–∏–Ω –æ—Ç–∫—Ä—ã—Ç—ã–π –≤–æ–ø—Ä–æ—Å. –í–æ–ø—Ä–æ—Å –Ω–∞—á–Ω–∏ —Å–æ —Å–ª–æ–≤: "–ê –∑–Ω–∞–µ—à—å –ª–∏ —Ç—ã", –∑–∞—Ç–µ–º —Ç–≤–æ–π –≤–æ–ø—Ä–æ—Å –∏ –≤ –∫–æ–Ω—Ü–µ —É—Ç–æ—á–Ω–∏: "–µ—Å–ª–∏ –Ω–µ –∑–Ω–∞–µ—à—å, —è –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —Ç–µ–±–µ —Ä–∞—Å—Å–∫–∞–∂—É!" """
            llm_new_question = format_llm_prompt(new_query)

            if event_id:
                count = 0
                for token in llm_new_question:
                    user_states[user_id]['init_message'] += token
                    count+= 1 
                    if count % token_speed == 0:
                        self.update_reply(message, user_states[user_id]['init_message'], event_id)
                        user_states[user_id]['init_message'] = user_states[user_id]['init_message']
                            
                    
                if user_states[user_id]['init_message'] != user_states[user_id]['last_updated_messsage']:
                    self.update_reply(message, user_states[user_id]['init_message'], event_id)

                '''new_mes_ending = f"""\n```spoiler  ‚õî Debug info:
{new_query}
```
"""'''
                new_mes_ending = ""

                self.update_reply(message, user_states[user_id]['init_message'] + new_mes_ending, event_id)

                #base llm without rag and agent

                base_user_query = f"""#–ü–æ–º–Ω–∏, —á—Ç–æ –µ—Å—Ç—å –∏—Å—Ç–æ—Ä–∏—è —Ä–∞–∑–≥–æ–≤–æ—Ä–∞: {str(user_history[user_id])}
# –í–≤–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:
- –¢—ã –ø—Ä–µ–ø–æ–¥–æ–≤–∞—Ç–µ–ª—å-–ø—Ä–æ—Ñ–µ—Å—Å–æ—Ä –≤ –í—ã—Å—à–µ–π —à–∫–æ–ª–µ —ç–∫–æ–Ω–æ–º–∏–∫–∏ –∏ –∫ —Ç–µ–±–µ –æ–±—Ä–∞—â–∞—é—Ç—Å—è —Å—Ç—É–¥–µ–Ω—Ç—ã –ø–æ —Ç–µ–º–µ –∫—É—Ä—Å–∞ "{topic_name}".
- –¢—ã –¥–æ–ª–∂–µ–Ω –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å —Å—Ç—É–¥–µ–Ω—Ç—É –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ –ø–æ–¥–¥–µ—Ä–∂–∞—Ç—å –±–µ—Å–µ–¥—É –ø–æ —Ç–µ–º–µ –∫—É—Ä—Å–∞, –µ—Å–ª–∏ –≤–æ–ø—Ä–æ—Å–∞ –Ω–µ—Ç.
- –°—Ç—É–¥–µ–Ω—Ç —Ç–µ–±–µ –Ω–∞–ø–∏—Å–∞–ª: {content}.
- –î–ª—è –æ—Ç–≤–µ—Ç–∞ –∏—Å–ø–æ–ª—å–∑—É–π {model_level_desc}.
- –ü–û–ú–ù–ò, —á—Ç–æ –Ω—É–∂–Ω–æ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—É—é —Ç–µ–±–µ –ø–æ –∫—É—Ä—Å—É.
- –î–ª–∏–Ω–∞ —Ç–≤–æ–µ–≥–æ –æ—Ç–≤–µ—Ç–∞ –∏ —Ç–µ–∫—Å—Ç–∞ –Ω–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –±–æ–ª—å—à–µ 2-3 –∞–±–∑–∞—Ü–µ–≤.
# –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç–≤–µ—Ç–∞:
- –û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å —Å—Ç—É–¥–µ–Ω—Ç–∞ (–≤ —Ä–∞–º–∫–∞—Ö –∫—É—Ä—Å–∞) –∏–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ —Ä–∞–∑–≥–æ–≤–æ—Ä —Å–æ —Å—Ç—É–¥–µ–Ω—Ç–æ–º (–≤ —Ä–∞–º–∫–∞—Ö –∫—É—Ä—Å–∞) —Å–∞–º—ã–º –ª—É—á—à–∏–º –æ–±—Ä–∞–∑–æ–º, –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –∏ –ª–æ–≥–∏—á–Ω–æ.
- –¢–≤–æ–π –æ—Ç–≤–µ—Ç –ù–ï –î–û–õ–ñ–ï–ù –ü–†–ï–í–´–®–ê–¢–¨ –ø–æ –æ–±—ä–µ–º—É 2-3 –∞–±–∑–∞—Ü–µ–≤.
"""
                
                user_states[user_id]['base_response'] = ''

                base_llm = format_llm_prompt(base_user_query)

                for token in base_llm:
                    user_states[user_id]['base_response'] += token

                
                # Define the new data you want to add
                new_entry = {
                    'time': datetime.now().isoformat(),
                    'user_id': str(message["sender_email"]),  # Replace with actual user ID
                    'question': str(content),
                    'rag_chunks': str(chunks_for_llm),
                    'theme': str(topic),  # Replace with actual theme
                    'theme_name': str(topic_name),  # Replace with actual theme
                    'level': str(level),  # Replace with actual level
                    'llm_base': str(user_states[user_id]['base_response']),  # Replace with actual LLM base
                    'llm_rag': str(user_states[user_id]['to_be_checked_by_agent']),  # Replace with actual LLM RAG
                    'llm_agent': str(user_states[user_id]['agents_get_response'])  # Replace with actual LLM agent
                }

                # Call the function to append data
                append_to_json_file(json_file_path, new_entry)





handler_class = QuizBot