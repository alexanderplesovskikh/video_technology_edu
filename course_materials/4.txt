# Приложения

# FFMPEG

::: info
Эта статья требует базовых знаний про работу с FFMPEG. Ожидается, что читатель освоил курс Компьютерная графика (модуль 3).

:::

Основная информация FFmpeg — программа с набором библиотек для изменения форматов медиаданных, а также записи и стриминга аудио и видео. FFmpeg помогает в различных повседневных операциях, как обрезка видео, получение аудио из видео, конвертирование из avi в mp4, так и в ситуациях посложнее, например, наложение изображения или другого видео в углу текущего, добавление эффектов и многое многое другое.

# Структура команды

Базовая команда начинается с ключевого слова “ffmpeg”. Далее ключ “-i” указывает источник. Источником может быть файл (видео, фото, аудио), поток и устройство. После прописывается название конечного файла или потока. Для файла обязательно указывается расширение.

```
ffmpeg -i input.avi result.mp4
```

Если же указать источник без выходного файла, то ffmpeg даст информацию об этом источнике, такую как разрешение сторон, количество кадров в секунду, длина, битрейт и т.д.

::: info
ffmpeg -i input.mp4

:::

![Рисунок 1.](.attachments.55572/pasted%20image%200.png)

Схематично, программа командной строки FFmpeg ожидает, что следующий формат аргументов выполнит свои действия — ffmpeg {1} {2} -i {3} {4} {5}, где:

{1} — глобальные параметры

{2} — параметры входного файла

{3} — входящий URL

{4} — параметры выходного файла

{5} — исходящий URL

В частях {2}, {3}, {4}, {5} может указываться несколько аргументов.

Процесс транскодирования в ffmpeg следует по следующей схеме:

![Рисунок 2.](.attachments.55572/pasted%20image%200%20%281%29.png)

С помощью декодера эти пакеты превращаются в несжатые данные (например, RAW-видео). После происходит обратный процесс: информация сжимается кодером и полученные пакеты мультиплексируются в конечный файл.

# Применение

В этом разделе приводится несколько примеров применения FFMPEG с фильтрами и потоками.

## Кодирование

Видеокадр кодируется в видеофайле не в экранных пикселях, а в своих собственных, "виртуальных" пикселях. А видеопроигрыватель преобразует эти виртуальные пиксели в экранные.

Полезно знать следующие сокращения и их значения:

* PAR, Pixel Aspect Ratio. Если PAR равен 1:1, то пиксель квадратный.
* SAR, Storage Aspect Ratio, то есть соотношение сторон кадра, записанного в текущем файле. PAR - это про пиксель, а SAR и DAR - это про кадр.  
  Высчитывается SAR просто путем деления ширины кадра, в своих "виртуальных" пикселях, к высоте кадра, тоже в своих "виртуальных" пикселях. Размер кадра 1920х1080? Значит SAR=16:9.
* DAR, Display Aspect Ratio, то есть соотношение сторон кадра, которое должно использоваться при выводе на экран. Если SAR и DAR не совпадают (и это частая ситуация), то видеоплеер должен сделать следующее: взять изображение в "виртуальных" пикселях, и при выводе их на экран растянуть картинку так, чтобы получилось соотношение сторон, заданное в DAR. Иногда это растягивание не увеличивает пропорционально, а деформирует до заполнения экрана полностью из-за неправильного считывания пропорций кадра.

Параметры PAR, SAR и DAR всегда связаны между собой простой формулой:

PAR = DAR / SAR

Мы хотим увидеть картинку в разрешении 640 на 480 то есть в соотношении сторон 4:3. Для этого DAR должен быть именно 640:480=1,3. По формуле ниже определяем, что SAR надо установить равным 1:1.

DAR = HORIZONTAL_RESOLUTION / VERTICAL_RESOLUTION \* SAR

Команда:

```
ffmpeg -i input.mp4 -vf scale=640:480,setsar=1:1 -r 30 -ss 2 -t 10 -c:v libx264 -b:v 1.5M -c:a aac -b:a 160k -cutoff 48000 output.mp4
```

* -vf (video filter) – ключ, отвечающий за изменение параметров кадров между процессами декодирования и кодирования;
* scale=640:480,setsar=1:1 – приводит видео к заданному размеру, а setsar указывает, что “виртуальный” пиксель будет квадратным;
* -r  – (frame Rate) ключ, устанавливающий частоту кадров в секунду
* -ss 2 – (Set Start) начинает записывать видео со 2 секунды Есть два варианта использования данного ключа:

  Когда нужно начать читать исходный файл с определенного момента. В связи с особенностями различных форматов, ffmpeg чаще всего будет находить приблизительно значение, после которого начнётся считывание.  
  Когда этот параметр стоит до выходного файла, то он будет декодироваться, но отбрасываться до нужного значения (после которого нужно начать запись)
* -t – (Time) ключ, определяющий сколько будет длиться видео

Здесь есть разница в зависимости от расположения ключа в команде.

* Если он стоит до источника, то программа ограничит количество данных, считываемых на входе.
* Когда ключ используется перед выходным файлом, программа останавливает запись после достижения заявленного значения.

```
ffmpeg -i input.mp4 -vf scale=640:480,setsar=1:1 -r 30 -ss 2 -t 10 -c:v libx264 -b:v 1.5M -c:a aac -b:a 160k -cutoff 48000 output.mp4
```

![Рисунок 3.](.attachments.55572/pasted%20image%200%20%284%29.png)

```
ffmpeg  -t 10 -ss 2 -i input.mp4 -vf scale=640:480,setsar=1:1 -r 30 -c:v libx264 -b:v 1.5M -c:a aac -b:a 160k -cutoff 48000 output.mp4 
```

![Рисунок 4.](.attachments.55572/pasted%20image%200%20%285%29.png)

В данном случае немного уменьшилось количество обрабатываемых кадров в секунду.

- -c:v – (enCode Video) кодирует видеопоток в соответствии с указанным кодеком (первая буква “с” означает сокращенную версию ключа “-codec”, а буква “v” работу с видеофайлом). Вы можете встретить другую нотацию -vcodec / -acodec -- этот способ записи считается устаревшим, предпочтительнее писать сответственно  -c:v / -c:a  или -с, когда нужно указать и аудио, и видео.

Ключ scale  в предыдущем примере приведет размер кадра к указанному:

```
ffmpeg -i input.mp4 -vf scale=1280:720 output.mp4
```

## Нарезка и склейка

Как обрезать видео без перекодирования?

Для указания начала и конца фрагмента, который нужно получить на выходе, используются соответственно ключи -ss и -t для указания длительности фрагмента или -to для указания таймкода конца фрагмента.  Параметр copy заставляет ffmpeg пропускать этап декодирования и кодирования, содержимое копируется без изменений -- это выполняется со скоростью копирования файла и не нагружает процессор:

```
ffmpeg -ss [start_timecode] -i in.mp4 -t [duration] -c copy out.mp4   
```

```
ffmpeg -ss [start_timecode] -i in.mp4 -to [end_timecode] -c copy out.mp4
```

Как обрезать видео с перекодированием?

Если не прописать -c copy, то параметры кодирования будут взяты из указанных в команде ключей, а недостающие -- из параметров по умолчанию. Например, в следующей команде кодеки и битрейт аудио указаны, а битрейт видео ffmpeg выберет на своё усмотрение. Обязательно указывайте битрейт и другие критичные для вас параметры, чтобы исключить неподходящих для вас значений в итоговом файле или потоке.

```
ffmpeg -ss [start] -i in.mp4 -t [duration] -c:v libx264 -c:a aac -strict experimental -b:a 128k out.mp4\n
```

Как соединить два видео без перекодирования?

Объединение файлов выполняется при помощи конкатенации в ffmpeg.

Для начала создайте текстовый файл, содержащий все файлы, которые вы хотите объединить:

```
file '/path/to/file1.wav'
file '/path/to/file2.wav'
file '/path/to/file3.wav'
```

* Файлы желательно располагать в одной папке.
* Важно, чтобы файлы были с одним и тем же кодеком
* Объединение выполняется без перекодирования (-c copy):

```
ffmpeg -f concat -safe 0 -i mylist.txt -c copy output.wav
```

* Значение -safe 0 не требуется, если пути совпадают.

## Потоки и фильтры

FFMPEG позволяет выполнять сложные комплексные команды с несколькими источниками и путями обработки их потоков. Для этого используется концепция цепочки фильтров (filterchain) — это когда фильтры перечислены через запятую. Например:

```
-vf rotate=PI/2,crop=600:600,hflip
```

Когда цепочки фильтров разделены точкой с запятой, то они образуют граф фильтров (filtergraph);

В каких же случаях использовать -vf, а в каких -filter_complex?

* -vf используется в случаях, когда в графе есть только один вход и только один выход,
* -filter_complex — когда есть один или несколько входов и один или несколько выходов.

К примеру,

```
ffmpeg -i input.mp4 -i logo.png -i logo2.png -filter_complex "[1:v]rotate=PI,hflip[logo];[0:v][logo]overlay=10:10[a];[a][2:v]overlay=main_w-overlay_w-10:main_h-overlay_h-10" result.mp4
```

В этом случае получается граф:

![Рисунок 5.](.attachments.55572/pasted%20image%200%20%282%29.png)

* \[0:v\] – В ffmpeg все входные файлы пронумерованы. Первый символ (0) означает номер входного файла (отсчёт начинается с нуля), второй (v) определяет тип файла, в данном случае это v - video.

В основном фильтры принимают на вход один видеопоток и отдают на выходе один видеопоток. Записывается это так:

```
-filter_complex "[1:v]hflip[logo]"
```

Сначала идёт входной поток, потом название фильтра и его параметры (или цепочки фильтров), в конце -- название выходного файла.

В данном случае, когда после наложения фильтра на второй входной файлы результат будет помещён в поток с названием logo. Добавим ещё один фильтр:

```
-filter_complex "[1:v]rotate=PI,hflip[logo]"
```

Теперь второй входной файл будет повёрнут на 180° и отражён по-горизонтали.

Есть фильтры, которые принимают два и более потоков, а на выходе производят один поток. К таким относится фильтр overlay, который принимает два видеопотока.

```
-filter_complex"[1:v]rotate=PI,hflip[logo];[0:v][logo]overlay=10:10[a]"
```

Фильтр берёт первый входной файл (\[0:v\]) и повёрнутый логотип (\[logo\]), размещает логотип в координатах 10:10 (левый верхний угол), а результат переходит в именованный поток \[a\].

overlay – с помощью данного фильтра можно наложить логотип или видео.

Фильтр принимает следующие значения:

main_w / W — ширина исходного видео

main_h / H — высота исходного видео

overlay_w / w — ширина накладываемого видео или изображения

overlay_h / h — высота накладываемого видео или изображения

Вернёмся к предыдущей теме про соединение видео.

Как соединить два видео с перекодированием?

```
ffmpeg -i input1.mp4 -i input2.webm -i input3.mov -filter_complex "[0:v:0][0:a:0][1:v:0][1:a:0][2:v:0][2:a:0]concat=n=3:v=1:a=1[outv][outa]" -map "[outv]" -map "[outa]" output.mkv
```

\[0:v:0\]\[0:a:0\]\[1:v:0\]\[1:a:0\]\[2:v:0\]\[2:a:0\] – эта строка сообщает ffmpeg, какие потоки следует брать из входных файлов и отправлять в качестве данных в фильтр concat (объединять). В этом случае видеопоток 0 \[0:v:0\] и аудиопоток 0 \[0:a:0\] с нулевого входного файла (input1.mp4 в этом примере), а видеопоток 0 \[1:v:0\] и аудиопоток 0 \[1:v:0\] с первого входного файла (input2.webm) и т.д.

```
concat=n=3:v=1:a=1[outv][outa]
```

Фильтр concat объединяет видео. n=3 говорит фильтру, что есть три входящих потока, v=1 означает, что на один поток будет одно видео, a=1 тоже самое только с аудио. Фильтр объединяет три сегмента и выдаёт на выход два потока: видеопоток и аудиопоток – \[outv\] и \[outa\]

```
-map "[outv]" -map "[outa]" output.mkv
```

Это указывает ffmpeg использовать результаты фильтра concat, а не потоки непосредственно из входных файлов.

![Рисунок 6.](.attachments.55572/pasted%20image%200%20%283%29.png)

## Потоки

Выбор потоков может потребоваться для создания копии файла с нужными потоками или для случаев, когда нужно скопировать видео или аудио из файла.

Форматы:

* -map \[индекс\] — выбирает все потоки указанного файла (-map 1)
* -map \[индекс:тип\_потока\] — выбирает все потоки заданного типа (-map 1:v)
* -map \[индекс:номер\_поток\] — выбирает конкретный поток по номеру (-map 1:0)
* -map \[индекс:тип\_потока:номер\_потока\] — выбирает конкретный поток по номеру и с указанным типом (-map 1:v:0)

В качестве индекса может быть число или название потока, которое задается в -vf / -filter_complex. Если указать отрицательный индекс, поток будет исключён (-map -0:v) Виды потоков:

* v (video) — видео (включая обложки, статичные изображения)
* V (video) — видео (только видео, не изображения)
* a (audio) — аудио
* s (subtitles) — субтитры
* d (data) — данные (не очень понимаю, что это, мб это вообще убрать)
* t (attachment) — вложения (как и это) Есть видео без звука и отдельная звуковая дорожка в mp3. Объединить эти два потока в видео со звуком: \\nffmpeg -i input.mp4 -i audio.mp3 -c:v copy -map 1:a result.mp4

Выход пайплайна В качестве выхода может быть не только файл, но и адрес потокового сервера. Например, вам даны видео и путь для доступа к rtmp серверу. Необходимо зациклить видео так, чтобы получилась непрерывная трансляция и застримить её на вышеупомянутый сервер.

1. input.mp4 - видео
2. rtmp://localhost:1935/stream/input - путь

```
ffmpeg -re -stream_loop -1 -i input.mp4 -c copy -f flv rtmp://localhost:1935/stream/input
```

* -re – считывает входные данные с собственной частотой кадров
* -stream_loop -1 – Заданное количество раз входной поток будет зациклен.
* 0 означает отсутствие цикла,
* -1 означает бесконечный цикл.
* -f – указывает формат выходного файла

ODM (ONVIF Device Manager) представляет собой специализированный инструмент для управления устройствами видеонаблюдения, поддерживающими стандарт ONVIF.

#### Основные возможности ODM

1. Подключение к устройствам ONVIF:  
   Пользователь может добавлять устройства вручную, вводя их IP-адрес, логин и пароль, либо использовать функцию автоматического поиска устройств в сети.  
   Автоматическое обнаружение камер и кодеров (WS Discovery) действует только в том же сегменте локальной сети, куда подключен ODM. Для других сегментов необходимо вводить адреса и номера портов ONVIF вручную.
2. Просмотр видео в реальном времени:  
   ODM предоставляет удобный интерфейс для просмотра видеопотоков с подключенных камер в режиме реального времени. Поддерживается работа с несколькими камерами (переключение между доступными камерами), выбор потоков для каждой камеры.
3. Управление настройками камеры:  
   С помощью ODM пользователи могут изменять параметры камер, доступные для изменения через ONVIF (состав этих параметров зависит от конкретной камеры).  
   Доступно управление PTZ-функциями (Pan-Tilt-Zoom).
4. Удаленное управление:  
   ODM может работать с устройствами, находящимися вне локальной сети (требуется проброс портов RTSP и ONVIF или VPN соединение с сетью, где установлены камеры).
5. Бесплатное использование:  
   Важным преимуществом ODM является то, что она распространяется бесплатно.

#### Принцип работы ODM

Принцип работы ODM основан на использовании протокола ONVIF, который определяет набор стандартов для взаимодействия различных устройств видеонаблюдения. ODM использует следующие компоненты:

- Discovery: механизм обнаружения устройств в сети. ODM автоматически находит все устройства, поддерживающие ONVIF, и добавляет их в список доступных.
- Device Management: управление параметрами устройств, включая изменение настроек камеры, запись видео, просмотр архива и т.д.
- Media Service: передача мультимедийного контента от устройств к клиенту (в данном случае – программе ODM).
- Event Service: обработка событий, таких как обнаружение движения, срабатывание датчиков и других триггеров.
- PTZ Control: управление поворотными функциями камер, включая панорамирование, наклон и зум.

VMix – это профессиональное программное обеспечение для видеопроизводства, которое позволяет создавать высококачественные видео трансляции, микшируя несколько источников контента в реальном времени. Программа работает только под Windows.

### Основные Возможности VMix

#### 1. Поддерживаемые источники видео

- VMix поддерживает широкий спектр входных устройств: 
  - Веб-камеры
  - Профессиональные карты видеозахвата SDI/HDMI
  - Потоки NDI (Network Device Interface)
  - RTSP и SRT IP-камеры и кодеры
  - Видеозвонки (собственная система связи Vmix Call на основе WebRTC)
  - Потоки с видеоплатформ через встроенный браузер
  - Захват экрана

#### 2. Эффекты и переходы

- VMix предлагает большое количество эффектов и переходов между источниками: 
  - Более 100 предустановленных переходов (слайд-шоу, растворение, вращение и др.)
  - Возможность создания пользовательских переходов
  - Эффект «картинка-в-картинке» (PiP)
  - Хромакей (зеленый экран)
  - Цветокоррекция и фильтры

#### 3. Титры и графика

- Поддержка статичных и анимированных титров
- Импорт графических файлов (PNG, JPG, GIF, BMP)
- Создание динамических титров с использованием HTML/CSS
- Загрузка шаблонов титров из библиотеки VMix
- "Виртуальная студия" (эффект)

#### 4. Запись и трансляция

- Запись видео в различных форматах (MP4, AVI, MPEG-2, WMV)
- Прямая трансляция на популярные платформы или на указанные адреса RTMP серверов (до трёх, в новых версиях -- до пяти).
- Одновременная запись и трансляция
- Регулировка битрейта и качества видео

#### 5. Аудио обработка

- Микширование аудио сигналов от разных источников
- Автоматическое переключение источника звука вслед за переключением видеоисточника
- Настройка уровней громкости и эквалайзера
- Подавление шума и эхо

#### 6. Интерфейсы и управление

- Поддержка внешних пультов управления (X-Keys, MIDI контроллеры)
- Управление через веб-интерфейс (для удаленного контроля)
- HTTP API
- Скриптовая автоматизация задач (VBScript)
- Визуальная автоматизация (триггеры)

### Лицензии VMix

VMix доступен в нескольких версиях, каждая из которых имеет свои ограничения и особенности:

- Basic: Бесплатная версия с ограничением до 4 входов и разрешением 768x576 пикселей.
- HD: Включает до 6 входов, разрешение до 1920x1080 пикселей и возможность использования Chroma Key.
- 4K: До 12 входов, разрешение до 4096x2160 пикселей, поддержка всех основных функций.
- Pro: Максимальная версия с неограниченным количеством входов, всеми функциями и возможностью работы с 8K контентом.

Каждая версия лицензируется на один компьютер и может использоваться как для личного, так и коммерческого применения.

### Работа с Дополнительным Оборудованием

VMix поддерживает интеграцию с различными устройствами для расширения возможностей и удобства работы:

#### 1. Пульты управления

- VMix совместим с рядом популярных пультов управления, таких как X-Keys и MIDI контроллеры.
- Пользователь может настроить кнопки пульта под конкретные задачи, такие как переключение камер, запуск эффектов или активация макросов.
- Пульт управления позволяет значительно ускорить процесс видеопроизводства и повысить удобство работы режиссера монтажа.

#### 2. Источники видео

- Для подключения профессиональных видеокамер и других внешних устройств используются карты видеозахвата HDMI и SDI.
- Профессиональные сетапы обычно используют SDI камеры и компьютер с VMix, оснащенный картами захвата SDI. Но с распространением NDI надобность в таких картах уходит, видеокомплексы переходят на IP-стек и обходятся высокоскоростной компьютерной сетью на витой паре.
- VMix поддерживает подключение IP камер через протоколы RTSP, SRT и NDI.

### Поддержка IP Источников

#### 1. RTSP (Real Time Streaming Protocol)

- Протокол RTSP используется для передачи потокового видео и аудио данных по IP-сетям. IP камеры видеонаблюдения передают сигнал через этот протокол, что позволяет включать их потоки как источники в VMix.
- Важно учитывать, что в реализации VMix потоки RTSP от камер могут иметь непредсказуемую задержку (обычно от 500 до 1000 мс), которая меняется со временем. Потоки от видеокодеров и вовсе нестабильны и приходится подключать их через встраиваемый модуль VLC, который хорошо обрабатывает входящие RTSP потоки, но дает ещё большую задержку (обычно больше 1000 мс). Поскольку при использовании камер наблюдения совместно с кодером звук обычно подключается к кодеру, приходится задерживать все потоки до значений, свойственных кодеру. Управлять камерами из VMix при такой задержку уже становится сложно.
- Для уверенной работы с RTSP потоками в программном микшере лучше выбирать OBS Studio с подключением потоков через плагин Gstreamer. Там задержки декодирования потоков будут сопоставимы с NDI.

#### 2. NDI (Network Device Interface)

- VMix полностью поддерживает NDI, что обеспечивает минимальную задержку видеопотока, но для его работы критична надежная и высокоскоростная локальная сеть (от 1 Гбит/с и выше).

#### 3. ONVIF (Open Network Video Interface Forum)

- Стандарт ONVIF\*\* не поддерживается в VMix\*\* из принципиальных соображений разработчиков VMix. Программа позиционируется для профессиональной телевизионной съемки и разработчики указывают на то, что камеры телевизионного класса управляются по VISCA или NDI, а ONVIF -- стандарт камер видеонаблюдения, хотя и поддерживается, например, PTZ камерами видеоконференцсвязи, вполне применимыми (и чаще всего используемыми) в  стриминге и записи мероприятий, как в составе переносных комплексов, так и в стационарно оснащенных помещениях.

VLC Media Player — это не только плеер мультимедийных файлов, он также позволяет работать с видео- и аудиопотоками через командную строку.

#### Основные возможности VLC при работе с командной строкой:

1. Запуск видеофайлов и их трансляция  
   VLC позволяет запускать медиафайлы и транслировать их (RTP, RTSP, HTTP и UDP.
2. Захват экрана  
   С помощью VLC можно захватить содержимое рабочего стола и передать его в виде потока.
3. Транскодирование потоков  
   Возможность изменять параметры видеопотока, такие как разрешение, битрейт, кодек и формат контейнера.
4. Запись потоков  
   Запись входящего видеопотока в файл для последующего использования.

---

#### Примеры команд для работы с видеопотоками

##### Захват экрана и передача по RTP:

```bash
vlc screen:// :screen-fps=30 :screen-caching=100 --sout '#transcode{vcodec=h264,vb=800,fps=25,width=1280,height=720}:rtp{dst=239.255.12.42,port=1234,sdp=rtsp://192.168.1.10:8080/test.sdp}'
```

Этот пример показывает, как захватить экран компьютера и отправить его по протоколу RTP на указанный IP-адрес и порт. Параметры vcodec, vb и fps определяют кодек, битрейт и частоту кадров соответственно.

##### Стриминг файла по RTSP:

```bash
vlc /path/to/video.mp4 --sout '#transcode{vcodec=h264,vb=2000,fps=24,width=640,height=480}:rtp{sdp=rtsp://localhost:554/stream.sdp}'
```

Эта команда транслирует локальный видеофайл по протоколу RTSP. Здесь задаются параметры кодека, битрейта, разрешения и частоты кадров.

##### Транскодирование и запись видеопотока:

```bash
vlc rtsp://example.com/live.stream --sout '#transcode{vcodec=mp4v,vb=1024,fps=15,aenc=none}:std{access=file,mux=ts,dst=/path/to/output.ts}'
```

В этом примере происходит прием потока RTSP, его транскодирование с использованием кодека  MPEG-4 Visual и сохранение в файл формата TS.

##### Захват веб-камеры и стриминг по HTTP:

```bash
vlc v4l2:///dev/video0 --sout '#transcode{vcodec=theo,vb=512,fps=20,width=320,height=240}:http{mux=ogg,dst=:8080/stream.ogv}'
```

Здесь производится захват видео с веб-камеры (/dev/video0) и его трансляция по протоколу  HTTP с использованием кодека Theora.

---

#### Заключение

VLC является мощным инструментом для работы с видеопотоками, позволяя инженерам по компьютерным сетям легко создавать, обрабатывать и передавать потоки по различным протоколам. Использование командной строки открывает широкие возможности для автоматизации процессов трансляции и записи медиа, делая VLC незаменимым средством для решения множества задач в области сетевых технологий.

# Gstreamer

В этом курсе важнейшими инструментами, помимо OBS, для нас станут FFMPEG и GStreamer.

С FFMPEG те, кто изучали курс компьютерной графики, знакомы. Только там мы не работали с потоками, а здесь это будет его основное применение.

В работе с FFMPEG прекрасно то, что этот инструмент хорошо документирован и вы легко найдете решения для большинства задач в документации или в профильных форумах.

С gstreamer всё сложнее. Вряд ли вы легко загуглите что-то за пределами основ. И в этом курсе совсем  глубоко мы погрузиться не сможем, освоим основные концепции и самое необходимое для работы с потоками.

Если FFMPEG -- это инструмент, которым можно легко пользоваться из командной строки и можно обходиться совсем без программирования, то GStreamer мы будем использовать из программ, которые вы будете писать на питоне.

Почему объединили эти два инструмента и чем нам не хватает уже знакомого FFMPEG?

Начнем с принципиальных отличий.

Одно я уже назвал: с GStreamer мы будем работать или из OBS, где он вызывается через плагин, или из программ, которые вы будете писать сами.

Второе -- это возможности, про них поподробнее.

* Непосредственно в курсе мы будем использовать способность gstreamer работать с потоками RTSP в реальном времени без буферизации или с предельно маленьким буфером -- в пределах  сотни миллисекунд. Решения на основе FFMPEG менее стабильны и работают с кратно большими задержками -- на практике от 500 миллисекунд и более.
* Вторая возможность -- менять параметры выполнения программы во время ее исполнения. Это уже посложнее. Если для изменения параметров работы ффмпег нужно остановить и перезапустить его с новыми параметрами, то GStreamer можно дать другие параметры на ходу. Например, это можно использовать для ручного или автоматического переключения источника потока. Правда, для этого придется писать программу, а не запускать из командной строки или пакетного файла, как это делали с FFMPEG.

В основном мы будем использовать GStreamer для работы с потоками по протоколу RTSP. По нему отдают видео IP камеры и кодеры.

# Что такое GStreamer

::: info
GStreamer -- это мультимедийный фреймворк для создания потоковых медиа приложений.

:::

Его основа - это видеопайплайн. Одной из причин для начала его разработки в 1999 году была бедность мультимедиа в Linux. Кстати, многие мультимедийные проекты были начаты около 1999-2000 года, в том числе FFmpeg, что говорит о наличии проблем достаточно красноречиво. И, хотя GStreamer начинался как инструмент для создания видео и аудио проигрывателей, он хорош в задачах, где важна низкая задержка и высокая эффективность приложений, то есть, для трансляций. Сообщество вокруг этого фреймворка живо и спустя более 20 лет.

# Зачем он нужен и где используется

::: info
Список программ, которые используют GStreamer.

:::

Библиотеки с фреймворком GStreamer существуют под C, C++, Python, Ruby и т.д. Код в методичке, которая прикреплена после урока, написан на C, так как GStreamer ориентирован на концепции этого языка, да и написан тоже на нем. Для несложных задач возможно использовать и другие языки.

Эта статья частично является переводом уроков с официального сайта этого фреймворка, у некоторых из них есть варианты кода на python и javascript, при желании можете изучить и их.

Также существует отладочный инструмент gst-launch-1.0, который нежелательно использовать при сборке своих приложений. Понимание этого инструмента позволит лучше разобраться в том, что такое пайплайн и как его составлять, поможет отлаживать программы, а также пригодится для использовании плагина GStreamer для OBS.

# Терминология

::: warn
В этой теме мы даже больше, чем обычно, будем сталкиваться с неустоявшейся русской терминологией.

:::

Это общая беда многих технических областей, где вся документация написана на английском, а в русской версии либо нет общепринятых терминов, либо они не менее иностранные. Например, в компьютерных сетях вас могут поправить, что вместо „свич“ надо говорить „коммутатор“, вместо „роутер“ -- „маршрутизатор“, вместо „фаерволл“ -- „брендмауэр.

Мы тоже будем заменять используемые в документации английские слова на чуть более понятные, но не менее английские. Например, „пэды“ будем переводить как „коннекторы“. Просто потому, что „PAD“ -- это не подушка, не блокнот и не коврик в нашем случае, а действительно место соединения элементов, коннектор.

Мы встречали эти термины в документах и в форумах и надеемся, что вам поможет знание этой терминологии. Тем не менее, старайтесь запоминать оригинальные термины, это поможет при поиске.

# Пайплайн

::: info
Пайплайн — это набор из готовых элементов, соединенных последовательно, где выход одного элемента является входом другого.

:::

Элементы являются базовыми „кирпичиками“, из которых строятся пайплайны в GStreamer. Они обрабатывают данные, которые исходят из элементов-источников и приходят в элементы-потребители (получатели), проходя через элементы фильтры. Чем элемент-источник отличается от фильтра или получателя? Это разделение происходит по наличию коннекторов (pads).

::: info
Коннектор - это составляющая элемента, с помощью которых эти самые элементы соединяются. Коннекторы бывают двух видов:

:::

* входы, или потребители (sink), куда поступает информационный поток, и
* выходы, или источники (src), которые такой поток отдают.

У элементов-источников есть только выход -- коннектор-источник.  
Такие элементы являются только источниками информационных потоков, на вход ничего не принимают.

![Рисунок 1.](.attachments.55443/image1.png)

У элементов-потребителей есть только вход - коннектор-получатель.  
Такие элементы только принимают информацию, но из них ничего нельзя получить.

![Рисунок 2.](.attachments.55443/image2.png)

У элементов-фильтров есть вход и выход.  
То есть, для предыдущего элемента в пайплайне он является приёмником, а для следующего - источником.

![Рисунок 3.](.attachments.55443/image3%20%282%29.png)

В итоге, элементы можно соединить таким образом:

![Рисунок 4.](.attachments.55443/image9.png)

или таким:

![Рисунок 5.](.attachments.55443/image4%20%282%29.png)

Примеры выше являются упрощенными по сравнению с реальными пайплайнами. Код и схему одного из них вы видите на ниже.

```gstreamer
gst-launch-1.0
    audiotestsrc ! voaacenc ! mux. 
    v4l2src device='/dev/video0' io-mode=2 ! image/jpeg, width=1280, height=720, framerate=30/1 !
    nvjpegdec ! 'video/x-raw, width=1280, height=720, framerate=30/1' !
    nvvidconv ! 'video/x-raw(memory:NVMM), width=1280, height=720, framerate=30/1' ! queue max-size-bytes=0 max-size-buffers=0 ! glue.     
    rtspsrc location='rtsp://<camera_ip>/Channel1 ' protocols=tcp latency=500 ! rtph264depay ! h264parse ! nvv4l2decoder !
    nvvidconv left=420 right=1500 top=0 bottom=1080 ! 'video/x-raw(memory:NVMM), width=1920, height=1080, framerate=25/1' ! queue max-size-bytes=0 max-size-buffers=0 ! glue. 
    videotestsrc pattern=2 is-live=true ! autovideoconvert ! glue. 
    nvcompositor name=glue \\n    sink_0::xpos=691 sink_0::ypos=195 sink_0::width=1220 sink_0::height=691 
    sink_1::xpos=0 sink_1::ypos=195 sink_1::width=691 sink_1::height=691 
    sink_2::xpos=0 sink_2::ypos=886 sink_2::width=1920 sink_2::height=194 
    background-w=1920 background-h=1080 background=0 ! 
nvvidconv ! 'video/x-raw(memory:NVMM), width=1920, height=1080, framerate=25/1' ! nvv4l2h264enc bitrate='4500000' !
    h264parse config-interval=1 disable-passthrough=true ! queue max-size-bytes=0 max-size-buffers=0 ! mux.
    flvmux name=mux streamable=true ! rtmpsink location='rtmp://a.rtmp.youtube.com/live2/<stream_key> live=1 flashver=FME/3.0\20(compatible;\20FMSc\201.0)' &> debug_strm_cam+screen.txt
```

![Рисунок 6. ](.attachments.55443/image10.png)

При работе с GStreamer не менее важным умением, чем программировать на C, является искусство поиска в интернете.

Но вместо того, чтобы каждый раз искать информацию о нужном элементе, можно использовать команду gst-inspect-1.0. Используется она следующим образом:

gst-inspect-1.0 <название элемента>

В выводе данной команды можно увидеть, что делает данный элемент, узнать, какие у него коннекторы и свойства, которые ему можно задать и т.д.

## Возможности - caps

У коннекторов есть capabilities (caps). Можно перевести как возможности или характеристики. Это тип данных, с которыми работает данный коннектор.

Рассмотрим на примере коннектора элемента x264enc, это кодировщик H264. У его коннектора-получателя возможности это video/x-raw, он ждет на вход сырой видеопоток, чтобы его закодировать. А возможности коннектора-источника -  video/x-h264, то есть элемент, который стоит за ним, получит закодированный поток, так что, скорее всего, это будет мультиплексор, про который подробнее поговорим далее.

Если вы подадите на вход элементу неправильный тип данных, он не сможет их обработать и пайплайн не запустится.

![Рисунок 7.](.attachments.55443/image12.png)

Помимо коннекторов, у элементов есть свойства (properties), которые задаются через пробел после названия элемента в формате <название свойства>=<значение>.

Свойства можно посмотреть через команду gst-inspect-1.0.

## Элементы-фильтры

Начнем с мультиплексоров и демультиплексоров (muxer/demuxer). Чтобы понять, что это за элементы и зачем они нужны, необходимо понимать, что такое медиаконтейнеры.

::: info
Медиаконтейнер, это такая сущность, внутри которой есть:

метаданные: например, информация о кодеке, количество потоков в файле, размер, имя и дата создания файла, название контента, одним слов, данные о данных внутри контейнера, и

медиапотоки: то есть видео- и аудиодорожки, а иногда и субтитры.

:::

::: info
Посмотрите это видео чтобы узнать больше о контейнерах и медиаформатах.

:::

Чтобы извлечь из контейнера эти медиапотоки нам пригодится демультиплексор. Аналогично, для „упаковки” данных в контейнер, необходимо использовать мультиплексор. Зачем нужно „открывать” контейнер? Почему нельзя работать с потоками напрямую? Поток, выходящий из файла, является одним целым, и, если захотите его как-то преобразовать, например, уменьшить размер кадра видео, то сделать у вас этого не получится.

На входе у такого элемента один коннектор, а на выходе столько, сколько дорожек было внутри контейнера(в примере две дорожки - для звука и для видео):

![Рисунок 8.](.attachments.55443/image5.png)

В этом случае при обработке пайплайна появляются две ветки:

одна - для обработки аудио, вторая - для обработки видео:

![Рисунок 9.](.attachments.55443/image7.png)

Вот как выглядит этот пайплайн при работе с gst-launch:

```
gst-launch-1.0 filesrc location=file://<path> ! oggdemux name=sep ! vorbisdec ! autoaudiosink sep. ! theoradec ! autovideosink
```

Здесь:

* демультиплексору дано имя через свойство name.
* Дальше идет часть пайплайна для ветки в аудио.
* Затем, после конца этой ветки, через пробел пишем имя с точкой.
* После точки можно написать название конкретного получателя.  Продолжаем ветку для видео.

## Кодирование видео

### Пример кодирования видео

Из тестового источника выходит сырой поток и он кодируется кодеком H.264.

```
gst-launch-1.0 -v videotestsrc ! x264enc ! mp4mux ! filesink location=res.mp4
```

* videotestsrc открывается тестовый поток
* x264enc поток сжимается кодеком H.264,
* mp4mux упаковывается в контейнер mp4,
* filesink location=res.mp4 потребитель (sink), в свойствах -- путь к выходному файлу.

### Пример декодирования видео

```
gst-launch-1.0 -v filesrc location=example.mkv ! matroskademux ! avdec_h264 ! ximagesink 
```

(пример для Linux)

## Упаковщики / распаковщики (pay/depay)

Элементы типа pay/depay чем-то схожи с (де)мультиплексорами, только они достают данные не из контейнеров, а из пакетов.

Например, rtph264depay достает видео, закодированное по стандарту H264 из rtp пакетов, которые используются, например, в протоколе RTSP при получении видео с камер или кодеров. Если захотите что-нибудь сделать с таким видео, его нужно будет декодировать перед обработкой.

Пример:

```
gst-launch-1.0 rtspsrc location="rtsp://<IP-адрес>/<путь>"  ! rtph264depay ! h264parse ! rtspclientsink location="rtsp://<IP-адрес>/<путь>"
```

## Парсеры

::: info
Парсеры (parse) обрабатывают полученные данные и разделяют их на отдельные видео/аудио кадры и метаданные (пакетированные закодированные данные со временными отметками, где возможно) и добавляют необходимые для кодирования/декодирования данные.

:::

Пример:

```
gst-launch-1.0 rtspsrc location="rtsp://<IP-адрес>/<путь>"  ! rtph264depay ! h264parse ! rtspclientsink location="rtsp://<IP-адрес>/<путь>"
```

# Полезные элементы пайплайна

* Элемент playbin: если задать свойство uri - это может быть файл, RTSP-поток или какое-нибудь видео, доступное по http, то он автоматически составит пайплайн из источников, декодеров, демультиплексоров и получателей для проигрывания этого медиа на экране.

  ```
  gst-launch-1.0 playbin uri=file://hehe.mkv
  ```
* Элемент uridecodebin декодирует данные из URI в сырое медиа. Он выбирает элемент-источник, который может „захватить“ данные по этому URI и соединяет его с элементом decodebin. Выступает в качестве демультиплексора, поэтому предлагает много коннекторов-источников, опираясь на потоки, которые найдет в медиа.

  ```
  gst-launch-1.0 uridecodebin uri=https://www.freedesktop.org/software/gstreamer-sdk/data/media/sintel_trailer-480p.webm ! videoconvert ! autovideosink
  ```
* Элемент decodebin автоматически создает пайплайн для декодирования, используя имеющиеся декодеры и демультиплексоры, пока не получится сырой медиапоток. Он используется внутри uridecodebin, который  создает подходящий элемент-источник.

  ```
  gst-launch-1.0 souphttpsrc location=https://www.freedesktop.org/software/gstreamer-sdk/data/media/sintel_trailer-480p.webm ! decodebin ! autovideosink
  ```
* Элемент videoscale позволяет изменять разрешение видео:

  ```
  gst-launch filesrc location=videofile.mov ! decodebin ! videoscale ! video/x-raw-yuv,width=640,height=340 ! autovideosink
  ```
* Элемент videocrop обрезает видеокадр:

  ```
  gst-launch-1.0 -v videotestsrc ! videocrop top=42 left=1 right=4 bottom=0 ! ximagesink
  ```

![Рисунок 10.](.attachments.55443/image11.png)

* Элемент compositor позволяет расположить несколько видео на одном полотне. пример:

  ```
  gst-launch-1.0 compositor name=comp \
  sink_0::alpha=1 sink_0::xpos=0 sink_0::ypos=0 \
  sink_1::alpha=0.5 sink_1::xpos=320 sink_1::ypos=0 ! \
  queue2 ! decodebin ! video/x-raw, width=800, height=600 ! videoconvert ! xvimagesink \
  videotestsrc pattern=1 ! "video/x-raw" ! comp.sink_0 \
  filesrc location=tst.mp4 ! decodebin ! videoconvert ! comp.sink_1
  ```

# Многопоточность

## Очереди (queue)

Использование очередей (queue) позволяет распараллелить вычисления и буферизировать информацию в автоматическом режиме для передачи между элементами.

Очереди выполняют две задачи:

1. Данные отправляются в очередь, пока она не достигнет выбранного предела.  
   Любая попытка добавить еще буферов в очередь блокирует процесс, который пытается это сделать, пока не появится свободное место.
2. Очередь создает новую нить процесса на коннекторе источника, чтобы разъединить обработку на коннекторах-получателях и на источниках.

Помимо этого, очередь отправляет сигналы о том, когда она становится пустой или полной (зависит от конфигурации) и может иметь инструкции, чтобы отбросить буферы вместо блокирования процессов при переполнении.

Пример:

1. Без очереди:

   ```
   gst-launch-1.0 videotestsrc ! autovideosink
   ```

   ![Рисунок 11.](.attachments.55443/image.png)
2. С очередью:

   ```
   gst-launch-1.0 videotestsrc ! queue ! autovideosink
   ```

![Рисунок 12.](.attachments.55443/image%20%282%29.png)

## Очереди (queue2)

Queue2 не является улучшенной версией queue: исполняет те же задачи, что и queue, а также может хранить полученные данные (или их часть) в файле, для последующего восстановления. Также он замещает сигналы более общими и удобными сообщениями.

К сожалению, не всегда можно просто сказать, какой из вариантов лучше использовать.

::: success
Старайтесь использовать queue2, когда пайплайн обрабатывает  
не сетевые потоки. Например, файл.

:::

::: success
Старайтесь использовать queue, когда пайплайн обрабатывает  
сетевые потоки. Например, поток RTSP.

:::

## Разветвители (tee)

Элемент tee (тройник) копирует поток, который в него приходит, и передает его в точку с соответствующим именем. \\nПример использования: нужно скопировать видео и поместить копию рядом с оригиналом.

![Рисунок 13.](.attachments.55443/image13.png)

```
gst-launch-1.0 \ 
videotestsrc ! tee name=t \
t. ! queue ! videoconvert ! autovideosink \
t. ! queue ! videoconvert ! autovideosink 
```

OBS Studio (Open Broadcaster Software, OBS) – это бесплатное программное обеспечение с открытым исходным кодом, которое позволяет пользователям захватывать, микшировать и транслировать аудио- и видеоконтент в реальном времени.

OBS стал одним из самых популярных среди стримеров, блогеров и создателей контента благодаря своей гибкости, обширному функционалу и поддержке различных платформ. Помимо Windows, Mac и Linux на платформе X86/X64, OBS возможно запустить на платформе ARM. В частности, на одноплатных компьютерах типа OrangePi 5, что открывает для этого приложения новые перспективы применения. Это выгодно отличает OBS от его прямого конкурента -- коммерческого микшера VMix, который работает только под Windows.

#### Основные возможности OBS Studio:

 1. Захват экрана и окна приложений: OBS Studio позволяет захватывать весь экран компьютера, отдельные окна программ или выбранную область рабочего стола. Это делает его идеальным инструментом для создания обучающих видеороликов, проведения вебинаров или демонстрации игрового процесса.
 2. Поддержка различных источников видео и аудио: В OBS Studio можно добавлять множество источников, таких как веб-камеры, микрофоны, звуковые карты, а также различные устройства захвата видео. Это дает возможность создавать сложные сценарии трансляции, комбинируя несколько потоков видео и звука.
 3. Эффекты и фильтры: Программа поддерживает широкий спектр эффектов и фильтров, включая цветокоррекцию, размытие фона, наложение текста и изображений. Эти инструменты позволяют улучшить качество передаваемого сигнала и сделать трансляцию более профессиональной.
 4. Сцены и переходы: OBS Studio предоставляет возможность создавать сцены, каждая из которых может содержать разные источники видео и аудио. Переходы между сценами могут быть настроены различными способами, от простого затухания до сложных анимаций.
 5. Запись видео: Помимо потоковой передачи, OBS Studio позволяет записывать видео на жесткий диск компьютера. Записи могут быть сохранены в различных форматах, включая MP4, FLV и MKV. Всегда устанавлиявайте MKV как формат записи! Это позволит восстановить запись, если она прервется аварийно.
 6. Интеграция с популярными стриминговыми сервисами: OBS Studio поддерживает прямую трансляцию на такие платформы, как Twitch, YouTube, Facebook Live и другие. Пользователь может настроить параметры потока, такие как битрейт, разрешение и частота кадров, чтобы обеспечить наилучшее качество трансляции.
 7. Плагины и расширения: Благодаря открытому исходному коду, сообщество разработчиков постоянно создает новые плагины и расширения для OBS Studio. Это позволяет добавить дополнительные функции, такие как интеграция с другими приложениями, автоматизация задач и многое другое.
 8. Кросс-платформенность: OBS Studio доступен для Windows, macOS и Linux, что делает его универсальным решением для пользователей разных операционных систем.
 9. Удобство использования: Несмотря на свою мощь, интерфейс программы достаточно интуитивен и прост в освоении.
10. Настройка горячих клавиш: Для удобства работы можно назначить горячие клавиши для переключения сцен, запуска/остановки записи и других часто используемых действий.

#### Применение OBS Studio:

- Стриминг игр
- Создание обучающего контента
- Корпоративные мероприятия
- Музыкальные выступления

OBS позволяет создать полноценное рабочее место режиссера с просмотром всех входящих потоков на мультиэкране, управлении выходной программой с пульта или клавиатуры.

::: warn
Подключение пультов к OBS сложнее, чем в VMix, где MIDI-пульты поддерживаются не только на прием команд, но и на управление подсветкой.

:::

### Источники потока

В контексте курса Сетевых видеотехнологий OBS интересен своей поддержкой видеопотоков RTSP, NDI, SRT.

::: warn
Потоки RTSP не стоит подключать штатным способом, используйте плагин GStreamer для получения стабильного потока без задержек.

:::

Вот пример корректного пайплайна для использования в OBS Studio:

```bash
rtspsrc location="rtsp://username:password@ip_address:port/path_to_stream" latency=100 ! decodebin ! videoconvert ! appsink
```

Этот пайплайн включает следующие элементы:

1. rtspsrc — определяет источник RTSP-потока.
2. latency=100 — устанавливает задержку в 100 мс для улучшения стабильности потока.
3. decodebin — автоматически выбирает подходящий декодер для входящего потока.
4. videoconvert — преобразует видео в необходимый формат.
5. appsink — передает обработанный поток дальше в OBS Studio.

Чтобы использовать этот пайплайн в OBS Studio:

1. Установите плагин Gstreamer для OBS Studio.
2. В разделе Источники добавьте новый источник типа GStreamer Source.
3. В поле «Pipeline» вставьте приведенный выше пайплайн, заменив username, password, ip_address, port и path_to_stream на реальные значения вашего RTSP-потока.
4. Нажмите «OK», и ваш RTSP-поток будет отображаться в OBS Studio.