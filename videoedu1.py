import random
import uuid
import logging
from datetime import datetime
import zulip
import os
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
import faiss
from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain_core.documents import Document
import tqdm
from langchain_ollama.llms import OllamaLLM
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
import re
import torch
import gc
gc.collect()
torch.cuda.empty_cache()

base_dir = "/home/user/zulip-videoedu/python-zulip-api/zulip_bots/zulip_bots/bots/videoedu/"

print('model start loading')
model_ollama = "gemma2:latest"
#best model but we need more memory
model_ollama = "electromagneticcyclone/t-lite-q:3_k_m"
model_ollama = "llama3.1:8b-instruct-q4_K_S"
model_ollama = "owl/t-lite:latest"

init_ollama = OllamaLLM(
    model=model_ollama,
    streaming=True,
    callbacks=[StreamingStdOutCallbackHandler()]
)

#max_tokens=10,
#num_predict=10,

print('model end loading')

user_level = ''
user_theme = ''
user_id = -1

def extract_number(file_name):
    # Use regex to find the number in the file name
    match = re.search(r'(\d+)', file_name)
    return int(match.group(1)) if match else float('inf')  # Return a large number if no digit found

def check_folder_exists(folder_path):
    if os.path.exists(folder_path) and os.path.isdir(folder_path):
        print(f"The folder '{folder_path}' exists.")
        return True
    else:
        print(f"The folder '{folder_path}' does not exist.")
        return False
    
def get_chuncks(text_string, chunkssize, chunkoverlap, lengthfunc):
    text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=chunkssize,
    chunk_overlap=chunkoverlap,
    length_function=lengthfunc,
    is_separator_regex=False,)

    texts = text_splitter.create_documents([text_string])
    return [text.page_content for text in texts]

def read_first_lines(folder_path):
    first_lines = []
    all_lines = {}
    # Iterate through all files in the folder
    for file_name in sorted(os.listdir(folder_path), key=extract_number):
        # Check if the file has a .txt extension
        if file_name.endswith(".txt"):
            file_path = os.path.join(folder_path, file_name)
            
            # Open the file and read the first line
            with open(file_path, 'r', encoding='utf-8') as file:
                all_current_lines = file.readlines()
                if file_path in all_lines:
                    all_lines[file_path] += all_current_lines
                else:
                    all_lines[file_path] = all_current_lines

              
                first_line = all_current_lines[0].strip()  # Read and strip any trailing spaces/newlines
                print("file_path: ", file_path)
                print("first line: ", first_line)
                if first_line[0] == "#":
                    first_line = first_line[1:].strip()

                first_lines.append(first_line)
    
    return first_lines, all_lines

# Example usage
folder_path = base_dir + "course_materials"
lines, all_lines = read_first_lines(folder_path)

print(lines)
print(all_lines.keys())

lines_to_str = ''

all_theme_ids = []

for line_id in range(len(lines)):
    all_theme_ids.append(int(line_id+1))
    lines_to_str += str(line_id + 1) + ". " + lines[line_id] + "\n"


main_menu = f"""‚úã –ü—Ä–∏–≤–µ—Ç! –Ø –±–æ—Ç-–æ–±—É—á–∞—Ç–æ—Ä –ø–æ –í–∏–¥–µ–æ—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è–º üé•\n\n
–ü–æ–º–æ–≥—É —Ç–µ–±–µ —Ä–∞–∑–æ–±—Ä–∞—Ç—å—Å—è –≤–æ –≤—Å–µ—Ö –¥–µ—Ç–∞–ª—è—Ö –∫—É—Ä—Å–∞. **–î–ª—è –Ω–∞—á–∞–ª–∞ –≤—ã–±–µ—Ä–∏ —Ç–µ–º—É, –∫–æ—Ç–æ—Ä–∞—è —Ç–µ–±–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–∞:**\n
{lines_to_str}
\n
–û—Ç–ø—Ä–∞–≤—å –º–Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å–≤—É—é—â–∏–π –Ω–æ–º–µ—Ä —Ç–µ–º—ã, –Ω–∞–ø—Ä–∏–º–µ—Ä: **```1```** –∏–ª–∏ **```5```**\n\n
–ß—Ç–æ–±—ã –≤–µ—Ä–Ω—É—Ç—å—Å—è –≤ –≥–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é, –≤–≤–µ–¥–∏: **`–ø–æ–º–æ—â—å`**
"""

# Example usage
folder_path_faiss = base_dir + "faiss"
folder_exists = check_folder_exists(folder_path_faiss)

vs_name = base_dir + 'faiss/faiss_1_1'
embedding_function = HuggingFaceEmbeddings(model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')

if not folder_exists:
    print('start to create faiss')
    res = {}
    for key in all_lines:
        temp_res = ''
        for itemm in all_lines[key]:
            temp_res += itemm 
        if key in res:
            res[key] += temp_res
        else:
            res[key] = temp_res

    
    chunk_dict = {}
    for key in res:
        res_chunks = get_chuncks(res[key], 500, 50, len)
        if key in chunk_dict:
            chunk_dict[key] += res_chunks
        else:
            chunk_dict[key] = res_chunks

    chunk_key_pairs = []
    for key in chunk_dict:
        for chunk in chunk_dict[key]:
            chunk_key_pairs.append([key, chunk])
        

    
    embeddings = embedding_function.embed_documents('–≠—Ç–æ –ø—Ä–∏–º–µ—Ä –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.')
    index = faiss.IndexFlatL2(len(embeddings[0])) 
    embed_dim = len(embeddings[0])
    vector_store = FAISS(
            embedding_function=embedding_function,
            index=index,
            docstore=InMemoryDocstore(),
            index_to_docstore_id={},
        )

    list_langchain_docs = []
    for element in tqdm.tqdm(chunk_key_pairs):
        key, chunk = element[0], element[1]
        list_langchain_docs.append(Document(page_content=chunk, metadata={"source": key},))

    uuids = [i+1 for i in range(len(list_langchain_docs))]

    print("uuids: ", uuids)

    vector_store.add_documents(documents=list_langchain_docs, ids=uuids)
    vector_store.save_local(vs_name)

new_vector_store = FAISS.load_local(
    vs_name, embedding_function, allow_dangerous_deserialization=True
)


def search_chunks(query,k=5):
    results = new_vector_store.similarity_search(
    query,
    k=k
    )

    gotten_chunks = []
    for res in results[:-1]:
        if " - " + res.page_content not in gotten_chunks:
            gotten_chunks.append(" - " + res.page_content)

    context_faiss = "\n".join(chunk for chunk in gotten_chunks)
    return context_faiss

def search_chunks_new_knowledge(query,k=5):
    results = new_vector_store.similarity_search(
    query,
    k=k
    )

    to_put = " - " + results[-1].page_content

    context_faiss = to_put

    return context_faiss

def search_chunks_biblio(query,k=5):
    results = new_vector_store.similarity_search(
    query,
    k=k
    )

    gotten_chunks = []
    for res in results[:-1]:
        if " - " + res.page_content not in gotten_chunks:
            source_path = res.metadata['source']
            source_file = source_path.split("/")[-1].split(".txt")[0].strip()
            line_index = int(source_file) - 1
            source_line = lines[line_index]
            gotten_chunks.append(" - " + res.page_content + f"""\n*¬© –ò—Å—Ç–æ—á–Ω–∏–∫: {source_line}*\n---\n""")

    context_faiss = "\n".join(chunk for chunk in gotten_chunks)
    return context_faiss

def format_llm_prompt(query):
    response = init_ollama.invoke(query)
    #torch.cuda.empty_cache()
    return response


user_states = {}

class QuizBot:
    def initialize(self, bot_handler):
        self.bot_handler = bot_handler
        self.setup_logging()
        self.client = zulip.Client(config_file=base_dir + "zuliprc")

    def setup_logging(self):
        logging.basicConfig(level=logging.DEBUG, format='%(asctime)s %(levelname)s %(message)s')

    def send_reply(self, message, response):
      
        self.client.send_message({
            "type": "private",
            "to": message["sender_email"],
            "content": response,
        })

    def usage(self) -> str:
        pass

    def handle_message(self, message, bot_handler):
    
        user_id = message["sender_id"]
        content = message["content"].strip().lower()

        if user_id not in user_states:
            user_states[user_id] = {"state": "main_menu"}

        state = user_states[user_id]["state"]

        if content == "–ø–æ–º–æ—â—å":
            user_states[user_id] = {"state": "main_menu"}
            self.send_reply(message, main_menu)
            return

        if state == "main_menu":
            if re.match(r"^\d+$", content.strip()) and int(content.strip()) in all_theme_ids:
                user_states[user_id] = {"state": "select_level", "topic": int(content)}
                self.send_reply(message, "–ö–∞–∫–æ–π —É –≤–∞—Å —É—Ä–æ–≤–µ–Ω—å –∑–Ω–∞–Ω–∏–π –ø–æ —Ç–µ–º–µ? –í—ã–±–µ—Ä–∏, –≤–≤–µ–¥—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é —Ü–∏—Ñ—Ä—É: **`1. –Ω–∞—á–∞–ª—å–Ω—ã–π`** / **`2. —Å—Ä–µ–¥–Ω–∏–π`** / **`3. –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π`**.\n\n–ß—Ç–æ–±—ã –≤–µ—Ä–Ω—É—Ç—å—Å—è –≤ –≥–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é, –≤–≤–µ–¥–∏: **`–ø–æ–º–æ—â—å`**")
            else:
                self.send_reply(message, main_menu)

        elif state == "select_level":
            if content in ["1", "2", "3"]:
                user_states[user_id] = {"state": "chat", "topic": user_states[user_id]["topic"], "level": content}
                #open file
                #prompt model to summorize it
                topic = user_states[user_id]["topic"]

                file_path_summary = base_dir + 'course_materials/' + str(topic) + ".txt"

                with open(file_path_summary, 'r', encoding="utf-8") as sum_file:
                    sum_lines = sum_file.readlines()
                
                sum_firstlines = sum_lines[0]

                if sum_firstlines[0] == "#":
                    sum_firstlines = sum_firstlines[1:].strip()

                sum_restlines = " ".join(sum_lines[1:])

                summary_prompt = f"–°–¥–µ–ª–∞–π –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –¥–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –º–∞–∫—Å–∏–º—É–º –Ω–∞ –î–í–ê –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –ù–ï –ë–û–õ–¨–®–ï: {sum_restlines}"

                self.send_reply(message, "üï• –ü–æ–¥–æ–∂–¥–∏, —è –∏—â—É —Å–≤–æ–π –∫–æ–Ω—Å–ø–µ–∫—Ç–∏–∫ –ø–æ —Ç–µ–º–µ...")

                llm_summary = format_llm_prompt(summary_prompt)

                self.send_reply(message, f"**–¢–µ–º–∞: **{sum_firstlines}\n**–†–µ–∑—é–º–µ –ø–æ —Ç–µ–º–µ: **\n---\n{llm_summary}\n---\n**–î–∞–≤–∞–π –Ω–∞—á–Ω—ë–º –∏–∑—É—á–µ–Ω–∏–µ —Ç–µ–º—ã, —Ä–∞—Å—Å–∫–∞–∂–∏, —á—Ç–æ —Ç—ã —Å–ª—ã—à–∞–ª –ø–æ —ç—Ç–æ–π —Ç–µ–º–µ?**\n\n–î–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞ –≤ –º–µ–Ω—é –≤–≤–µ–¥–∏—Ç–µ **`–ø–æ–º–æ—â—å`**.")
            else:
                self.send_reply(message, "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—ã–±–µ—Ä–∏—Ç–µ —É—Ä–æ–≤–µ–Ω—å, –≤–≤–µ–¥—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é —Ü–∏—Ñ—Ä—É: **`1. –Ω–∞—á–∞–ª—å–Ω—ã–π`**, **`2. —Å—Ä–µ–¥–Ω–∏–π`** –∏–ª–∏ **`3. –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π`**.\n\n–ß—Ç–æ–±—ã –≤–µ—Ä–Ω—É—Ç—å—Å—è –≤ –≥–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é, –≤–≤–µ–¥–∏: **`–ø–æ–º–æ—â—å`**")

        elif state == "chat":
            topic = user_states[user_id]["topic"]
            level = user_states[user_id]["level"]
            topic_name = lines[int(str(topic).strip())-1]
            
            model_level_desc = ''
            if level == '–Ω–∞—á–∞–ª—å–Ω—ã–π':
                model_level_desc = '–æ–±—ä—è—Å–Ω–µ–Ω–∏—è –ø–æ–ø—Ä–æ—â–µ –¥–ª—è —Å—Ç—É–¥–µ–Ω—Ç–æ–≤ —Å –Ω–∞—á–∞–ª—å–Ω—ã–º —É—Ä–æ–≤–Ω–µ–º –∑–Ω–∞–Ω–∏–π'
            if level == '—Å—Ä–µ–¥–Ω–∏–π':
                model_level_desc = '—Å—Ä–µ–¥–Ω–∏–π —É—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –ª–µ–∫—Å–∏–∫–∏ - –¥–ª—è —Å—Ç—É–¥–µ–Ω—Ç–æ–≤ —Å—Ä–µ–¥–Ω–µ–≥–æ —É—Ä–æ–≤–Ω—è'
            if level == '–ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π':
                model_level_desc = '—Å–ª–æ–∂–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å –ª–µ–∫—Å–∏–∫–∏ –Ω–∞—É—á–Ω—ã–π –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–π —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–π —è–∑—ã–∫ - –¥–ª—è —Å—Ç—É–¥–µ–Ω—Ç–æ–≤ —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º —É—Ä–æ–≤–Ω–µ–º'

            #Main answer

            chunks_for_llm = search_chunks(content)

            user_query = f"""# –í–≤–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:
- –¢—ã –ø—Ä–µ–ø–æ–¥–æ–≤–∞—Ç–µ–ª—å-–ø—Ä–æ—Ñ–µ—Å—Å–æ—Ä –≤ –í—ã—Å—à–µ–π —à–∫–æ–ª–µ —ç–∫–æ–Ω–æ–º–∏–∫–∏ –∏ –∫ —Ç–µ–±–µ –æ–±—Ä–∞—â–∞—é—Ç—Å—è —Å—Ç—É–¥–µ–Ω—Ç—ã –ø–æ —Ç–µ–º–µ –∫—É—Ä—Å–∞ "{topic_name}".
- –¢—ã –¥–æ–ª–∂–µ–Ω –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å —Å—Ç—É–¥–µ–Ω—Ç—É –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ –ø–æ–¥–¥—Ä–µ–∂–∞—Ç—å –±–µ—Å–µ–¥—É –ø–æ —Ç–µ–º–µ –∫—É—Ä—Å–∞, –µ—Å–ª–∏ –≤–æ–ø—Ä–æ—Å–∞ –Ω–µ—Ç.
- –£ —Å—Ç—É–¥–µ–Ω—Ç–∞ —É–∫–∞–∑–∞–Ω —É—Ä–æ–≤–µ–Ω—å –∑–Ω–∞–Ω–∏–π, –∏—Å–ø–æ–ª—å–∑—É–π {model_level_desc}.
- –ü–û–ú–ù–ò, —á—Ç–æ –Ω—É–∂–Ω–æ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—É—é —Ç–µ–±–µ –ø–æ –∫—É—Ä—Å—É.
- –î–ª–∏–Ω–∞ —Ç–≤–æ–µ–≥–æ –æ—Ç–≤–µ—Ç–∞ –∏ —Ç–µ–∫—Å—Ç–∞ –Ω–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –±–æ–ª—å—à–µ 40 (—Å–æ—Ä–æ–∫) —Å–ª–æ–≤.
# –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç–≤–µ—Ç–∞:
- –û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å —Å—Ç—É–¥–µ–Ω—Ç–∞ (–≤ —Ä–∞–º–∫–∞—Ö –∫—É—Ä—Å–∞) –∏–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ —Ä–∞–∑–≥–æ–≤–æ—Ä —Å–æ —Å—Ç—É–¥–µ–Ω—Ç–æ–º (–≤ —Ä–∞–º–∫–∞—Ö –∫—É—Ä—Å–∞) —Å–∞–º—ã–º –ª—É—á—à–∏–º –æ–±—Ä–∞–∑–æ–º, –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –∏ –ª–æ–≥–∏—á–Ω–æ.
- –¢–≤–æ–π –æ—Ç–≤–µ—Ç –ù–ï –î–û–õ–ñ–ï–ù –ü–†–ï–í–´–®–ê–¢–¨ –ø–æ –æ–±—ä–µ–º—É 40 (—Å–æ—Ä–æ–∫) —Å–ª–æ–≤.
# –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –æ—Ç–≤–µ—Ç–∞:
- –í–æ–ø—Ä–æ—Å —Å—Ç—É–¥–µ–Ω—Ç–∞: {content}.
- –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ –∫—É—Ä—Å—É: {chunks_for_llm}.
"""

            chunks_get = search_chunks_biblio(content)

            self.send_reply(message, "üí≠...")

            llm_main_response = format_llm_prompt(user_query)

            response = f"""**Debug info:** –≤—ã –≤—ã–±—Ä–∞–ª–∏ —Ç–µ–º—É {topic}. {topic_name}. C —É—Ä–æ–≤–Ω–µ–º –∑–Ω–∞–Ω–∏–π: {level}. –í–∞—à –≤–æ–ø—Ä–æ—Å: {content}\n
{llm_main_response}\n
\n```spoiler  üìö –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏:
{chunks_get}
```
"""

            self.send_reply(message, response)

            

            #Agents start

            self.send_reply(message, "‚úÖ –ü–æ–¥–∫–ª—é—á–∞–µ–º –∞–≥–µ–Ω—Ç–æ–≤ –ø–æ –ø—Ä–æ–≤–µ—Ä–∫–µ –æ—Ç–≤–µ—Ç–æ–≤...")

            agents_prompt = f"""<text_to_check>
{llm_main_response}
</text_to_check>

<information_text_must_be_based_on>
{chunks_for_llm}
</information_text_must_be_based_on>

–ù–µ–æ–±—Ö–æ–¥–∏–º–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ç–µ–∫—Å—Ç, –∑–∞–∫–ª—é—á–µ–Ω–Ω—ã–π –º–µ–∂–¥—É —Ç–µ–≥–∞–º–∏ <text_to_check> –∏ </text_to_check> –Ω–∞ –µ–≥–æ —Å–æ–æ—Ç–≤–µ—Ç—Å–≤–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –∑–∞–∫–ª—é—á–µ–Ω–Ω–æ–π –º–µ–∂–¥—É —Ç–µ–≥–∞–º–∏ <information_text_must_be_based_on> –∏ </information_text_must_be_based_on>, –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ —Ç–µ–∫—Å—Ç –±—ã–ª –Ω–∞–ø–∏—Å–∞–Ω —Å –æ–ø–æ—Ä–æ–π –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∞ –¥–∞–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é. –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å–≤—É–µ—Ç –∑–∞–¥–∞–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, —Ç–æ —Ç—ã –¥–æ–ª–∂–µ–Ω –∏—Å–ø—Ä–∞–≤–∏—Ç—å —Ç–µ–∫—Å—Ç —Ç–∞–∫, —á—Ç–æ–±—ã –≤ –Ω–µ–º –±—ã–ª–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –¢–û–õ–¨–ö–û –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏ –ù–ò–ö–ê–ö–û–ô –î–†–£–ì–û–ô –í–ù–ï–®–ù–ï–ô –ò–ù–§–û–†–ú–ê–¶–ò–ò.
–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç–≤–µ—Ç–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Å–ª–µ–¥—É—é—â–µ–π: 1) –æ—Ü–µ–Ω–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Ç–µ–∫—Å—Ç–∞ –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ –∫–æ—Ç–æ—Ä–æ–π –æ–Ω –±—ã–ª –Ω–∞–ø–∏—Å–∞–Ω; 2) –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Å –æ–ø–æ—Ä–æ–π —Ç–æ–ª—å–∫–æ –Ω–∞ –¥–∞–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.
–¢–µ–∫—Å—Ç —è–≤–ª—è–µ—Ç—Å—è –æ—Ç–≤–µ—Ç–æ–º –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π –≤–æ–ø—Ä–æ—Å: {content}.
"""

            self.send_reply(message, format_llm_prompt(agents_prompt))

            #Question back

            self.send_reply(message, "üìù –§–æ—Ä–º–∏—Ä—É—é –ø–ª–∞–Ω –æ–±—Å—É–∂–¥–µ–Ω–∏—è...")

            new_know_chunks = search_chunks_new_knowledge(content)

            new_query = f"""–°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π –æ–¥–∏–Ω –æ—Ç–∫—Ä—ã—Ç—ã–π –≤–æ–ø—Ä–æ—Å –ø–æ –¥–∞–Ω–Ω–æ–π —Ç–µ–±–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –≤ –æ—Ç–≤–µ—Ç–µ —É–∫–∞–∂–∏ —Ç–æ–ª—å–∫–æ —Å–∞–º –≤–æ–ø—Ä–æ—Å. –ù–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –ø—ã—Ç–∞—Ç—å—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤—Å–µ —Ñ–∞–∫—Ç—ã –∏–∑ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –º–æ–∂–Ω–æ –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å—Å—è –ª–∏—à—å —á–∞—Å—Ç—å—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –ø–æ –Ω–µ–π —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –æ–¥–∏–Ω –æ—Ç–∫—Ä—ã—Ç—ã–π –≤–æ–ø—Ä–æ—Å. –í–æ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è: {new_know_chunks}"""

            self.send_reply(message, f"""{format_llm_prompt(new_query)}""")

            self.send_reply(message, f"""
\n```spoiler  ‚õî Debug info:
{new_query}
```
""")



handler_class = QuizBot